{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FJU9aA6rJ_ww"
      },
      "outputs": [],
      "source": [
        "# Tons and tons of imports!\n",
        "# from collections import Counter, namedtuple\n",
        "# from itertools import chain\n",
        "# import json\n",
        "# import math\n",
        "# import os\n",
        "# from pathlib import Path\n",
        "# from tqdm.notebook import tqdm, trange\n",
        "# from typing import List, Tuple, Dict, Set, Union\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.nn import init\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "# import torch.nn.utils\n",
        "# import torch.nn.functional as F\n",
        "# from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unDkKa--dGWG"
      },
      "source": [
        "**Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mQWFCYM3Lw-h"
      },
      "outputs": [],
      "source": [
        "LF_train = pd.read_csv('./raw_data/LF_train.csv')\n",
        "LH_train = pd.read_csv('./raw_data/LH_train.csv')\n",
        "# rf_train = pd.read_csv('./raw_data/RF_train.csv')\n",
        "# rf_test = pd.read_csv('./draw_ata/RH_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>weight</th>\n",
              "      <th>forceplate_date</th>\n",
              "      <th>age</th>\n",
              "      <th>LF</th>\n",
              "      <th>speed</th>\n",
              "      <th>V1_LF</th>\n",
              "      <th>V1_RF</th>\n",
              "      <th>V1_LH</th>\n",
              "      <th>...</th>\n",
              "      <th>V28_LH_trot</th>\n",
              "      <th>V28_RH_trot</th>\n",
              "      <th>V29_LF_trot</th>\n",
              "      <th>V29_RF_trot</th>\n",
              "      <th>V29_LH_trot</th>\n",
              "      <th>V29_RH_trot</th>\n",
              "      <th>V30_LF_trot</th>\n",
              "      <th>V30_RF_trot</th>\n",
              "      <th>V30_LH_trot</th>\n",
              "      <th>V30_RH_trot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>34.600</td>\n",
              "      <td>20190626</td>\n",
              "      <td>5.079452</td>\n",
              "      <td>1</td>\n",
              "      <td>1.315</td>\n",
              "      <td>183.831519</td>\n",
              "      <td>261.735816</td>\n",
              "      <td>173.086274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200878</td>\n",
              "      <td>0.255695</td>\n",
              "      <td>0.378632</td>\n",
              "      <td>0.375222</td>\n",
              "      <td>0.136255</td>\n",
              "      <td>0.109891</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.272321</td>\n",
              "      <td>0.232143</td>\n",
              "      <td>0.245536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>25.600</td>\n",
              "      <td>20191003</td>\n",
              "      <td>7.178082</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1400000000000001</td>\n",
              "      <td>178.404307</td>\n",
              "      <td>194.485630</td>\n",
              "      <td>115.977757</td>\n",
              "      <td>...</td>\n",
              "      <td>0.176045</td>\n",
              "      <td>0.163725</td>\n",
              "      <td>0.283679</td>\n",
              "      <td>0.339011</td>\n",
              "      <td>0.198281</td>\n",
              "      <td>0.179028</td>\n",
              "      <td>0.291317</td>\n",
              "      <td>0.282913</td>\n",
              "      <td>0.212885</td>\n",
              "      <td>0.212885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>36.700</td>\n",
              "      <td>20110425</td>\n",
              "      <td>1.772603</td>\n",
              "      <td>1</td>\n",
              "      <td>1.032</td>\n",
              "      <td>248.316618</td>\n",
              "      <td>245.881475</td>\n",
              "      <td>150.944727</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227326</td>\n",
              "      <td>0.193144</td>\n",
              "      <td>0.249856</td>\n",
              "      <td>0.313535</td>\n",
              "      <td>0.238719</td>\n",
              "      <td>0.197890</td>\n",
              "      <td>0.269729</td>\n",
              "      <td>0.254697</td>\n",
              "      <td>0.246764</td>\n",
              "      <td>0.228810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>64.411</td>\n",
              "      <td>20090715</td>\n",
              "      <td>5.608219</td>\n",
              "      <td>0</td>\n",
              "      <td>0.962</td>\n",
              "      <td>375.960740</td>\n",
              "      <td>373.157890</td>\n",
              "      <td>221.110528</td>\n",
              "      <td>...</td>\n",
              "      <td>0.220833</td>\n",
              "      <td>0.146660</td>\n",
              "      <td>0.272691</td>\n",
              "      <td>0.348814</td>\n",
              "      <td>0.238568</td>\n",
              "      <td>0.139927</td>\n",
              "      <td>0.268178</td>\n",
              "      <td>0.260623</td>\n",
              "      <td>0.228990</td>\n",
              "      <td>0.242210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>36.400</td>\n",
              "      <td>20190909</td>\n",
              "      <td>0.783562</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1066666666666667</td>\n",
              "      <td>251.456723</td>\n",
              "      <td>251.681851</td>\n",
              "      <td>134.528131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.178055</td>\n",
              "      <td>0.201276</td>\n",
              "      <td>0.465763</td>\n",
              "      <td>0.421552</td>\n",
              "      <td>0.183823</td>\n",
              "      <td>0.112685</td>\n",
              "      <td>0.301775</td>\n",
              "      <td>0.287968</td>\n",
              "      <td>0.177515</td>\n",
              "      <td>0.232742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>176</td>\n",
              "      <td>0</td>\n",
              "      <td>27.200</td>\n",
              "      <td>20080206</td>\n",
              "      <td>3.602740</td>\n",
              "      <td>0</td>\n",
              "      <td>1.034</td>\n",
              "      <td>172.448409</td>\n",
              "      <td>169.714560</td>\n",
              "      <td>138.505126</td>\n",
              "      <td>...</td>\n",
              "      <td>0.249825</td>\n",
              "      <td>0.232572</td>\n",
              "      <td>0.247507</td>\n",
              "      <td>0.254186</td>\n",
              "      <td>0.264056</td>\n",
              "      <td>0.234251</td>\n",
              "      <td>0.290997</td>\n",
              "      <td>0.290997</td>\n",
              "      <td>0.205252</td>\n",
              "      <td>0.212755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>28.200</td>\n",
              "      <td>20090211</td>\n",
              "      <td>1.978082</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1575</td>\n",
              "      <td>195.628736</td>\n",
              "      <td>200.532297</td>\n",
              "      <td>131.060970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.214868</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.279966</td>\n",
              "      <td>0.287559</td>\n",
              "      <td>0.215779</td>\n",
              "      <td>0.216696</td>\n",
              "      <td>0.286990</td>\n",
              "      <td>0.283163</td>\n",
              "      <td>0.211735</td>\n",
              "      <td>0.218112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>180</td>\n",
              "      <td>1</td>\n",
              "      <td>26.300</td>\n",
              "      <td>20080609</td>\n",
              "      <td>3.942466</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0779999999999998</td>\n",
              "      <td>169.406911</td>\n",
              "      <td>172.968121</td>\n",
              "      <td>110.135375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.226625</td>\n",
              "      <td>0.219313</td>\n",
              "      <td>0.254491</td>\n",
              "      <td>0.264647</td>\n",
              "      <td>0.243868</td>\n",
              "      <td>0.236994</td>\n",
              "      <td>0.274532</td>\n",
              "      <td>0.275099</td>\n",
              "      <td>0.230289</td>\n",
              "      <td>0.220079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>181</td>\n",
              "      <td>1</td>\n",
              "      <td>20.200</td>\n",
              "      <td>20090226</td>\n",
              "      <td>8.986301</td>\n",
              "      <td>0</td>\n",
              "      <td>1.188</td>\n",
              "      <td>128.704239</td>\n",
              "      <td>127.564176</td>\n",
              "      <td>81.999105</td>\n",
              "      <td>...</td>\n",
              "      <td>0.187021</td>\n",
              "      <td>0.197141</td>\n",
              "      <td>0.303722</td>\n",
              "      <td>0.298653</td>\n",
              "      <td>0.204484</td>\n",
              "      <td>0.193141</td>\n",
              "      <td>0.263977</td>\n",
              "      <td>0.277233</td>\n",
              "      <td>0.228242</td>\n",
              "      <td>0.230548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>22.600</td>\n",
              "      <td>20090128</td>\n",
              "      <td>1.890411</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1883333333333332</td>\n",
              "      <td>157.895443</td>\n",
              "      <td>156.617448</td>\n",
              "      <td>102.073996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.236740</td>\n",
              "      <td>0.225701</td>\n",
              "      <td>0.261071</td>\n",
              "      <td>0.261597</td>\n",
              "      <td>0.247756</td>\n",
              "      <td>0.229575</td>\n",
              "      <td>0.292986</td>\n",
              "      <td>0.297331</td>\n",
              "      <td>0.204221</td>\n",
              "      <td>0.205462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>110 rows × 368 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  gender  weight  forceplate_date       age  LF               speed  \\\n",
              "67    2       0  34.600         20190626  5.079452   1               1.315   \n",
              "76    3       1  25.600         20191003  7.178082   0  1.1400000000000001   \n",
              "94    6       0  36.700         20110425  1.772603   1               1.032   \n",
              "54    7       0  64.411         20090715  5.608219   0               0.962   \n",
              "11    8       0  36.400         20190909  0.783562   0  1.1066666666666667   \n",
              "..  ...     ...     ...              ...       ...  ..                 ...   \n",
              "18  176       0  27.200         20080206  3.602740   0               1.034   \n",
              "44  178       0  28.200         20090211  1.978082   0              1.1575   \n",
              "92  180       1  26.300         20080609  3.942466   0  1.0779999999999998   \n",
              "53  181       1  20.200         20090226  8.986301   0               1.188   \n",
              "48  184       0  22.600         20090128  1.890411   0  1.1883333333333332   \n",
              "\n",
              "         V1_LF       V1_RF       V1_LH  ...  V28_LH_trot  V28_RH_trot  \\\n",
              "67  183.831519  261.735816  173.086274  ...     0.200878     0.255695   \n",
              "76  178.404307  194.485630  115.977757  ...     0.176045     0.163725   \n",
              "94  248.316618  245.881475  150.944727  ...     0.227326     0.193144   \n",
              "54  375.960740  373.157890  221.110528  ...     0.220833     0.146660   \n",
              "11  251.456723  251.681851  134.528131  ...     0.178055     0.201276   \n",
              "..         ...         ...         ...  ...          ...          ...   \n",
              "18  172.448409  169.714560  138.505126  ...     0.249825     0.232572   \n",
              "44  195.628736  200.532297  131.060970  ...     0.214868     0.210482   \n",
              "92  169.406911  172.968121  110.135375  ...     0.226625     0.219313   \n",
              "53  128.704239  127.564176   81.999105  ...     0.187021     0.197141   \n",
              "48  157.895443  156.617448  102.073996  ...     0.236740     0.225701   \n",
              "\n",
              "    V29_LF_trot  V29_RF_trot  V29_LH_trot  V29_RH_trot  V30_LF_trot  \\\n",
              "67     0.378632     0.375222     0.136255     0.109891     0.250000   \n",
              "76     0.283679     0.339011     0.198281     0.179028     0.291317   \n",
              "94     0.249856     0.313535     0.238719     0.197890     0.269729   \n",
              "54     0.272691     0.348814     0.238568     0.139927     0.268178   \n",
              "11     0.465763     0.421552     0.183823     0.112685     0.301775   \n",
              "..          ...          ...          ...          ...          ...   \n",
              "18     0.247507     0.254186     0.264056     0.234251     0.290997   \n",
              "44     0.279966     0.287559     0.215779     0.216696     0.286990   \n",
              "92     0.254491     0.264647     0.243868     0.236994     0.274532   \n",
              "53     0.303722     0.298653     0.204484     0.193141     0.263977   \n",
              "48     0.261071     0.261597     0.247756     0.229575     0.292986   \n",
              "\n",
              "    V30_RF_trot  V30_LH_trot  V30_RH_trot  \n",
              "67     0.272321     0.232143     0.245536  \n",
              "76     0.282913     0.212885     0.212885  \n",
              "94     0.254697     0.246764     0.228810  \n",
              "54     0.260623     0.228990     0.242210  \n",
              "11     0.287968     0.177515     0.232742  \n",
              "..          ...          ...          ...  \n",
              "18     0.290997     0.205252     0.212755  \n",
              "44     0.283163     0.211735     0.218112  \n",
              "92     0.275099     0.230289     0.220079  \n",
              "53     0.277233     0.228242     0.230548  \n",
              "48     0.297331     0.204221     0.205462  \n",
              "\n",
              "[110 rows x 368 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### turn literals into numericals\n",
        "LF_train = LF_train.drop(columns=['dob', 'gait', 'Gait'])\n",
        "LF_train['forceplate_date'] = LF_train['forceplate_date'].str.replace(\"-\",\"\").astype(int)\n",
        "\n",
        "### nan value\n",
        "LF_train = LF_train.fillna(0)\n",
        "LF_train = LF_train.replace('Not able to trot', 0)\n",
        "LF_train['Speed'] = LF_train['Speed'].astype(float)\n",
        "\n",
        "for col in LF_train.columns:\n",
        "    if 'V' in col or 'Speed' in col :\n",
        "        LF_train[col] = LF_train[col].replace(0, LF_train[col].mean())\n",
        "\n",
        "### sort by id\n",
        "LF_train = LF_train.sort_values(by=['id'])\n",
        "\n",
        "### perform PCA and get rid of redundant columns\n",
        "\n",
        "LF_train.to_csv('LF_train.csv')\n",
        "LF_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>weight</th>\n",
              "      <th>forceplate_date</th>\n",
              "      <th>age</th>\n",
              "      <th>LH</th>\n",
              "      <th>speed</th>\n",
              "      <th>V1_LF</th>\n",
              "      <th>V1_RF</th>\n",
              "      <th>V1_LH</th>\n",
              "      <th>...</th>\n",
              "      <th>V28_LH_trot</th>\n",
              "      <th>V28_RH_trot</th>\n",
              "      <th>V29_LF_trot</th>\n",
              "      <th>V29_RF_trot</th>\n",
              "      <th>V29_LH_trot</th>\n",
              "      <th>V29_RH_trot</th>\n",
              "      <th>V30_LF_trot</th>\n",
              "      <th>V30_RF_trot</th>\n",
              "      <th>V30_LH_trot</th>\n",
              "      <th>V30_RH_trot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>24.700</td>\n",
              "      <td>20170510</td>\n",
              "      <td>10.558904</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.193317</td>\n",
              "      <td>0.213219</td>\n",
              "      <td>0.293557</td>\n",
              "      <td>0.312367</td>\n",
              "      <td>0.188542</td>\n",
              "      <td>0.205534</td>\n",
              "      <td>0.286856</td>\n",
              "      <td>0.287958</td>\n",
              "      <td>0.214935</td>\n",
              "      <td>0.210251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>25.600</td>\n",
              "      <td>20191003</td>\n",
              "      <td>7.178082</td>\n",
              "      <td>1</td>\n",
              "      <td>1.1400000000000001</td>\n",
              "      <td>178.404307</td>\n",
              "      <td>194.485630</td>\n",
              "      <td>115.977757</td>\n",
              "      <td>...</td>\n",
              "      <td>0.176045</td>\n",
              "      <td>0.163725</td>\n",
              "      <td>0.283679</td>\n",
              "      <td>0.339011</td>\n",
              "      <td>0.198281</td>\n",
              "      <td>0.179028</td>\n",
              "      <td>0.291317</td>\n",
              "      <td>0.282913</td>\n",
              "      <td>0.212885</td>\n",
              "      <td>0.212885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>36.200</td>\n",
              "      <td>20110307</td>\n",
              "      <td>8.391781</td>\n",
              "      <td>1</td>\n",
              "      <td>1.12</td>\n",
              "      <td>267.060125</td>\n",
              "      <td>266.926251</td>\n",
              "      <td>169.227812</td>\n",
              "      <td>...</td>\n",
              "      <td>0.196803</td>\n",
              "      <td>0.183123</td>\n",
              "      <td>0.313062</td>\n",
              "      <td>0.298513</td>\n",
              "      <td>0.195874</td>\n",
              "      <td>0.192550</td>\n",
              "      <td>0.277131</td>\n",
              "      <td>0.275904</td>\n",
              "      <td>0.232373</td>\n",
              "      <td>0.214592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>64.411</td>\n",
              "      <td>20090715</td>\n",
              "      <td>5.608219</td>\n",
              "      <td>1</td>\n",
              "      <td>0.962</td>\n",
              "      <td>375.960740</td>\n",
              "      <td>373.157890</td>\n",
              "      <td>221.110528</td>\n",
              "      <td>...</td>\n",
              "      <td>0.220833</td>\n",
              "      <td>0.146660</td>\n",
              "      <td>0.272691</td>\n",
              "      <td>0.348814</td>\n",
              "      <td>0.238568</td>\n",
              "      <td>0.139927</td>\n",
              "      <td>0.268178</td>\n",
              "      <td>0.260623</td>\n",
              "      <td>0.228990</td>\n",
              "      <td>0.242210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>36.400</td>\n",
              "      <td>20190909</td>\n",
              "      <td>0.783562</td>\n",
              "      <td>1</td>\n",
              "      <td>1.1066666666666667</td>\n",
              "      <td>251.456723</td>\n",
              "      <td>251.681851</td>\n",
              "      <td>134.528131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.201276</td>\n",
              "      <td>0.465763</td>\n",
              "      <td>0.421552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.112685</td>\n",
              "      <td>0.301775</td>\n",
              "      <td>0.287968</td>\n",
              "      <td>0.177515</td>\n",
              "      <td>0.232742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>177</td>\n",
              "      <td>1</td>\n",
              "      <td>20.100</td>\n",
              "      <td>20090305</td>\n",
              "      <td>3.323288</td>\n",
              "      <td>0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>148.599453</td>\n",
              "      <td>142.968998</td>\n",
              "      <td>97.123233</td>\n",
              "      <td>...</td>\n",
              "      <td>0.192578</td>\n",
              "      <td>0.208536</td>\n",
              "      <td>0.313399</td>\n",
              "      <td>0.281439</td>\n",
              "      <td>0.201820</td>\n",
              "      <td>0.203343</td>\n",
              "      <td>0.281919</td>\n",
              "      <td>0.284490</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.212511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>181</td>\n",
              "      <td>1</td>\n",
              "      <td>20.200</td>\n",
              "      <td>20090226</td>\n",
              "      <td>8.986301</td>\n",
              "      <td>0</td>\n",
              "      <td>1.188</td>\n",
              "      <td>128.704239</td>\n",
              "      <td>127.564176</td>\n",
              "      <td>81.999105</td>\n",
              "      <td>...</td>\n",
              "      <td>0.187021</td>\n",
              "      <td>0.197141</td>\n",
              "      <td>0.303722</td>\n",
              "      <td>0.298653</td>\n",
              "      <td>0.204484</td>\n",
              "      <td>0.193141</td>\n",
              "      <td>0.263977</td>\n",
              "      <td>0.277233</td>\n",
              "      <td>0.228242</td>\n",
              "      <td>0.230548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>182</td>\n",
              "      <td>1</td>\n",
              "      <td>32.700</td>\n",
              "      <td>20080429</td>\n",
              "      <td>2.326027</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0466666666666666</td>\n",
              "      <td>201.934578</td>\n",
              "      <td>201.288505</td>\n",
              "      <td>147.452671</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.453658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.469759</td>\n",
              "      <td>0.251149</td>\n",
              "      <td>0.306789</td>\n",
              "      <td>0.191935</td>\n",
              "      <td>0.250128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>183</td>\n",
              "      <td>0</td>\n",
              "      <td>36.500</td>\n",
              "      <td>20070716</td>\n",
              "      <td>3.208219</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>235.293254</td>\n",
              "      <td>251.611638</td>\n",
              "      <td>148.724997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.203955</td>\n",
              "      <td>0.211797</td>\n",
              "      <td>0.298878</td>\n",
              "      <td>0.272637</td>\n",
              "      <td>0.209835</td>\n",
              "      <td>0.218650</td>\n",
              "      <td>0.273709</td>\n",
              "      <td>0.278511</td>\n",
              "      <td>0.226090</td>\n",
              "      <td>0.221689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>22.600</td>\n",
              "      <td>20090128</td>\n",
              "      <td>1.890411</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1883333333333332</td>\n",
              "      <td>157.895443</td>\n",
              "      <td>156.617448</td>\n",
              "      <td>102.073996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.236740</td>\n",
              "      <td>0.225701</td>\n",
              "      <td>0.261071</td>\n",
              "      <td>0.261597</td>\n",
              "      <td>0.247756</td>\n",
              "      <td>0.229575</td>\n",
              "      <td>0.292986</td>\n",
              "      <td>0.297331</td>\n",
              "      <td>0.204221</td>\n",
              "      <td>0.205462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>110 rows × 368 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  gender  weight  forceplate_date        age  LH               speed  \\\n",
              "5     1       1  24.700         20170510  10.558904   1                   0   \n",
              "6     3       1  25.600         20191003   7.178082   1  1.1400000000000001   \n",
              "87    5       0  36.200         20110307   8.391781   1                1.12   \n",
              "92    7       0  64.411         20090715   5.608219   1               0.962   \n",
              "39    8       0  36.400         20190909   0.783562   1  1.1066666666666667   \n",
              "..  ...     ...     ...              ...        ...  ..                 ...   \n",
              "22  177       1  20.100         20090305   3.323288   0                1.16   \n",
              "64  181       1  20.200         20090226   8.986301   0               1.188   \n",
              "57  182       1  32.700         20080429   2.326027   0  1.0466666666666666   \n",
              "41  183       0  36.500         20070716   3.208219   0                   1   \n",
              "53  184       0  22.600         20090128   1.890411   0  1.1883333333333332   \n",
              "\n",
              "         V1_LF       V1_RF       V1_LH  ...  V28_LH_trot  V28_RH_trot  \\\n",
              "5     0.000000    0.000000    0.000000  ...     0.193317     0.213219   \n",
              "6   178.404307  194.485630  115.977757  ...     0.176045     0.163725   \n",
              "87  267.060125  266.926251  169.227812  ...     0.196803     0.183123   \n",
              "92  375.960740  373.157890  221.110528  ...     0.220833     0.146660   \n",
              "39  251.456723  251.681851  134.528131  ...     0.000000     0.201276   \n",
              "..         ...         ...         ...  ...          ...          ...   \n",
              "22  148.599453  142.968998   97.123233  ...     0.192578     0.208536   \n",
              "64  128.704239  127.564176   81.999105  ...     0.187021     0.197141   \n",
              "57  201.934578  201.288505  147.452671  ...     0.000000     0.453658   \n",
              "41  235.293254  251.611638  148.724997  ...     0.203955     0.211797   \n",
              "53  157.895443  156.617448  102.073996  ...     0.236740     0.225701   \n",
              "\n",
              "    V29_LF_trot  V29_RF_trot  V29_LH_trot  V29_RH_trot  V30_LF_trot  \\\n",
              "5      0.293557     0.312367     0.188542     0.205534     0.286856   \n",
              "6      0.283679     0.339011     0.198281     0.179028     0.291317   \n",
              "87     0.313062     0.298513     0.195874     0.192550     0.277131   \n",
              "92     0.272691     0.348814     0.238568     0.139927     0.268178   \n",
              "39     0.465763     0.421552     0.000000     0.112685     0.301775   \n",
              "..          ...          ...          ...          ...          ...   \n",
              "22     0.313399     0.281439     0.201820     0.203343     0.281919   \n",
              "64     0.303722     0.298653     0.204484     0.193141     0.263977   \n",
              "57     0.000000     0.530241     0.000000     0.469759     0.251149   \n",
              "41     0.298878     0.272637     0.209835     0.218650     0.273709   \n",
              "53     0.261071     0.261597     0.247756     0.229575     0.292986   \n",
              "\n",
              "    V30_RF_trot  V30_LH_trot  V30_RH_trot  \n",
              "5      0.287958     0.214935     0.210251  \n",
              "6      0.282913     0.212885     0.212885  \n",
              "87     0.275904     0.232373     0.214592  \n",
              "92     0.260623     0.228990     0.242210  \n",
              "39     0.287968     0.177515     0.232742  \n",
              "..          ...          ...          ...  \n",
              "22     0.284490     0.221080     0.212511  \n",
              "64     0.277233     0.228242     0.230548  \n",
              "57     0.306789     0.191935     0.250128  \n",
              "41     0.278511     0.226090     0.221689  \n",
              "53     0.297331     0.204221     0.205462  \n",
              "\n",
              "[110 rows x 368 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### turn literals into numericals\n",
        "LH_train = LH_train.drop(columns=['dob', 'gait', 'Gait'])\n",
        "LH_train['forceplate_date'] = LH_train['forceplate_date'].str.replace(\"-\",\"\").astype(int)\n",
        "\n",
        "### nan value\n",
        "LH_train = LH_train.fillna(0)\n",
        "# LH_train = LH_train.replace('Not able to trot', 0)\n",
        "# LH_train['Speed'] = LH_train['Speed'].astype(float)\n",
        "\n",
        "# for col in LH_train.columns:\n",
        "#     if 'V' in col or 'Speed' in col :\n",
        "#         LH_train[col] = LH_train[col].replace(0, LH_train[col].mean())\n",
        "\n",
        "### sort by id\n",
        "LH_train = LH_train.sort_values(by=['id'])\n",
        "\n",
        "### perform PCA and get rid of redundant columns\n",
        "\n",
        "LH_train.to_csv('LH_train.csv')\n",
        "LH_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "AhGRrh2L5PNI",
        "outputId": "2290d667-29fc-4f40-99ee-7f88e5af41b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/69/r1dg6qsd2_j9csr9658538qw0000gn/T/ipykernel_62573/1214989995.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  LF_train_trot['is_trot'] = 1\n"
          ]
        }
      ],
      "source": [
        "# split by walk / trot\n",
        "trot_list = []\n",
        "name_list = []\n",
        "for col in LF_train.columns:\n",
        "    if 'trot' in col:\n",
        "        trot_list.append(col)\n",
        "\n",
        "        new_name = col.split('_trot')\n",
        "        name_list.append(new_name[0])\n",
        "\n",
        "LF_train_trot = pd.DataFrame(LF_train, columns = trot_list)\n",
        "LF_train_trot.columns = name_list\n",
        "\n",
        "both = ['id', 'gender', 'weight', 'forceplate_date', 'speed', 'age', 'Speed', 'LF']\n",
        "LF_train_both = pd.DataFrame(LF_train, columns = both)\n",
        "\n",
        "# add binary col is_trot\n",
        "LF_train_trot = LF_train_both.join(LF_train_trot)\n",
        "LF_train_trot['is_trot'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/69/r1dg6qsd2_j9csr9658538qw0000gn/T/ipykernel_62573/3765687609.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  LF_train_walk['is_trot'] = 0\n"
          ]
        }
      ],
      "source": [
        "LF_train_walk = LF_train.drop(columns=trot_list)\n",
        "LF_train_walk['is_trot'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# concat walk and trot\n",
        "LF_train = pd.concat([LF_train_walk, LF_train_trot])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "rkl-LSqz5l7u",
        "outputId": "886be2f5-3ef5-41d1-e5fd-60f507baf235"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>weight</th>\n",
              "      <th>forceplate_date</th>\n",
              "      <th>age</th>\n",
              "      <th>LF</th>\n",
              "      <th>speed</th>\n",
              "      <th>V1_LF</th>\n",
              "      <th>V1_RF</th>\n",
              "      <th>V1_LH</th>\n",
              "      <th>...</th>\n",
              "      <th>V29_LF</th>\n",
              "      <th>V29_RF</th>\n",
              "      <th>V29_LH</th>\n",
              "      <th>V29_RH</th>\n",
              "      <th>V30_LF</th>\n",
              "      <th>V30_RF</th>\n",
              "      <th>V30_LH</th>\n",
              "      <th>V30_RH</th>\n",
              "      <th>Speed</th>\n",
              "      <th>is_trot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>34.6</td>\n",
              "      <td>20190626</td>\n",
              "      <td>5.079452</td>\n",
              "      <td>1</td>\n",
              "      <td>1.315</td>\n",
              "      <td>183.831519</td>\n",
              "      <td>261.735816</td>\n",
              "      <td>173.086274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.354105</td>\n",
              "      <td>0.485929</td>\n",
              "      <td>0.075465</td>\n",
              "      <td>0.084501</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.249135</td>\n",
              "      <td>0.262976</td>\n",
              "      <td>0.252595</td>\n",
              "      <td>1.710</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>34.6</td>\n",
              "      <td>20190626</td>\n",
              "      <td>5.079452</td>\n",
              "      <td>1</td>\n",
              "      <td>1.315</td>\n",
              "      <td>185.208712</td>\n",
              "      <td>249.535675</td>\n",
              "      <td>145.922523</td>\n",
              "      <td>...</td>\n",
              "      <td>0.378632</td>\n",
              "      <td>0.375222</td>\n",
              "      <td>0.136255</td>\n",
              "      <td>0.109891</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.272321</td>\n",
              "      <td>0.232143</td>\n",
              "      <td>0.245536</td>\n",
              "      <td>1.710</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>25.6</td>\n",
              "      <td>20191003</td>\n",
              "      <td>7.178082</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1400000000000001</td>\n",
              "      <td>178.404307</td>\n",
              "      <td>194.485630</td>\n",
              "      <td>115.977757</td>\n",
              "      <td>...</td>\n",
              "      <td>0.402185</td>\n",
              "      <td>0.492270</td>\n",
              "      <td>0.076547</td>\n",
              "      <td>0.028998</td>\n",
              "      <td>0.260504</td>\n",
              "      <td>0.266106</td>\n",
              "      <td>0.233894</td>\n",
              "      <td>0.239496</td>\n",
              "      <td>1.812</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>25.6</td>\n",
              "      <td>20191003</td>\n",
              "      <td>7.178082</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1400000000000001</td>\n",
              "      <td>280.954561</td>\n",
              "      <td>290.108186</td>\n",
              "      <td>144.419058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.283679</td>\n",
              "      <td>0.339011</td>\n",
              "      <td>0.198281</td>\n",
              "      <td>0.179028</td>\n",
              "      <td>0.291317</td>\n",
              "      <td>0.282913</td>\n",
              "      <td>0.212885</td>\n",
              "      <td>0.212885</td>\n",
              "      <td>1.812</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>36.7</td>\n",
              "      <td>20110425</td>\n",
              "      <td>1.772603</td>\n",
              "      <td>1</td>\n",
              "      <td>1.032</td>\n",
              "      <td>248.316618</td>\n",
              "      <td>245.881475</td>\n",
              "      <td>150.944727</td>\n",
              "      <td>...</td>\n",
              "      <td>0.303288</td>\n",
              "      <td>0.328469</td>\n",
              "      <td>0.177950</td>\n",
              "      <td>0.190293</td>\n",
              "      <td>0.250408</td>\n",
              "      <td>0.261827</td>\n",
              "      <td>0.259584</td>\n",
              "      <td>0.228181</td>\n",
              "      <td>2.026</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>180</td>\n",
              "      <td>1</td>\n",
              "      <td>26.3</td>\n",
              "      <td>20080609</td>\n",
              "      <td>3.942466</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0779999999999998</td>\n",
              "      <td>169.406911</td>\n",
              "      <td>172.968121</td>\n",
              "      <td>110.135375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.281286</td>\n",
              "      <td>0.276961</td>\n",
              "      <td>0.196854</td>\n",
              "      <td>0.244898</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.250388</td>\n",
              "      <td>0.239254</td>\n",
              "      <td>0.245987</td>\n",
              "      <td>2.030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>181</td>\n",
              "      <td>1</td>\n",
              "      <td>20.2</td>\n",
              "      <td>20090226</td>\n",
              "      <td>8.986301</td>\n",
              "      <td>0</td>\n",
              "      <td>1.188</td>\n",
              "      <td>211.831023</td>\n",
              "      <td>212.581617</td>\n",
              "      <td>131.925549</td>\n",
              "      <td>...</td>\n",
              "      <td>0.303722</td>\n",
              "      <td>0.298653</td>\n",
              "      <td>0.204484</td>\n",
              "      <td>0.193141</td>\n",
              "      <td>0.263977</td>\n",
              "      <td>0.277233</td>\n",
              "      <td>0.228242</td>\n",
              "      <td>0.230548</td>\n",
              "      <td>2.100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>181</td>\n",
              "      <td>1</td>\n",
              "      <td>20.2</td>\n",
              "      <td>20090226</td>\n",
              "      <td>8.986301</td>\n",
              "      <td>0</td>\n",
              "      <td>1.188</td>\n",
              "      <td>128.704239</td>\n",
              "      <td>127.564176</td>\n",
              "      <td>81.999105</td>\n",
              "      <td>...</td>\n",
              "      <td>0.346563</td>\n",
              "      <td>0.344794</td>\n",
              "      <td>0.157216</td>\n",
              "      <td>0.151427</td>\n",
              "      <td>0.262001</td>\n",
              "      <td>0.265471</td>\n",
              "      <td>0.234818</td>\n",
              "      <td>0.237710</td>\n",
              "      <td>2.100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>22.6</td>\n",
              "      <td>20090128</td>\n",
              "      <td>1.890411</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1883333333333332</td>\n",
              "      <td>157.895443</td>\n",
              "      <td>156.617448</td>\n",
              "      <td>102.073996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.424436</td>\n",
              "      <td>0.396303</td>\n",
              "      <td>0.109802</td>\n",
              "      <td>0.069459</td>\n",
              "      <td>0.252557</td>\n",
              "      <td>0.272041</td>\n",
              "      <td>0.238675</td>\n",
              "      <td>0.236727</td>\n",
              "      <td>2.142</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>22.6</td>\n",
              "      <td>20090128</td>\n",
              "      <td>1.890411</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1883333333333332</td>\n",
              "      <td>249.485374</td>\n",
              "      <td>250.633042</td>\n",
              "      <td>161.854799</td>\n",
              "      <td>...</td>\n",
              "      <td>0.261071</td>\n",
              "      <td>0.261597</td>\n",
              "      <td>0.247756</td>\n",
              "      <td>0.229575</td>\n",
              "      <td>0.292986</td>\n",
              "      <td>0.297331</td>\n",
              "      <td>0.204221</td>\n",
              "      <td>0.205462</td>\n",
              "      <td>2.142</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220 rows × 189 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  gender  weight  forceplate_date       age  LF               speed  \\\n",
              "67    2       0    34.6         20190626  5.079452   1               1.315   \n",
              "67    2       0    34.6         20190626  5.079452   1               1.315   \n",
              "76    3       1    25.6         20191003  7.178082   0  1.1400000000000001   \n",
              "76    3       1    25.6         20191003  7.178082   0  1.1400000000000001   \n",
              "94    6       0    36.7         20110425  1.772603   1               1.032   \n",
              "..  ...     ...     ...              ...       ...  ..                 ...   \n",
              "92  180       1    26.3         20080609  3.942466   0  1.0779999999999998   \n",
              "53  181       1    20.2         20090226  8.986301   0               1.188   \n",
              "53  181       1    20.2         20090226  8.986301   0               1.188   \n",
              "48  184       0    22.6         20090128  1.890411   0  1.1883333333333332   \n",
              "48  184       0    22.6         20090128  1.890411   0  1.1883333333333332   \n",
              "\n",
              "         V1_LF       V1_RF       V1_LH  ...    V29_LF    V29_RF    V29_LH  \\\n",
              "67  183.831519  261.735816  173.086274  ...  0.354105  0.485929  0.075465   \n",
              "67  185.208712  249.535675  145.922523  ...  0.378632  0.375222  0.136255   \n",
              "76  178.404307  194.485630  115.977757  ...  0.402185  0.492270  0.076547   \n",
              "76  280.954561  290.108186  144.419058  ...  0.283679  0.339011  0.198281   \n",
              "94  248.316618  245.881475  150.944727  ...  0.303288  0.328469  0.177950   \n",
              "..         ...         ...         ...  ...       ...       ...       ...   \n",
              "92  169.406911  172.968121  110.135375  ...  0.281286  0.276961  0.196854   \n",
              "53  211.831023  212.581617  131.925549  ...  0.303722  0.298653  0.204484   \n",
              "53  128.704239  127.564176   81.999105  ...  0.346563  0.344794  0.157216   \n",
              "48  157.895443  156.617448  102.073996  ...  0.424436  0.396303  0.109802   \n",
              "48  249.485374  250.633042  161.854799  ...  0.261071  0.261597  0.247756   \n",
              "\n",
              "      V29_RH    V30_LF    V30_RF    V30_LH    V30_RH  Speed  is_trot  \n",
              "67  0.084501  0.235294  0.249135  0.262976  0.252595  1.710        0  \n",
              "67  0.109891  0.250000  0.272321  0.232143  0.245536  1.710        1  \n",
              "76  0.028998  0.260504  0.266106  0.233894  0.239496  1.812        0  \n",
              "76  0.179028  0.291317  0.282913  0.212885  0.212885  1.812        1  \n",
              "94  0.190293  0.250408  0.261827  0.259584  0.228181  2.026        0  \n",
              "..       ...       ...       ...       ...       ...    ...      ...  \n",
              "92  0.244898  0.264371  0.250388  0.239254  0.245987  2.030        0  \n",
              "53  0.193141  0.263977  0.277233  0.228242  0.230548  2.100        1  \n",
              "53  0.151427  0.262001  0.265471  0.234818  0.237710  2.100        0  \n",
              "48  0.069459  0.252557  0.272041  0.238675  0.236727  2.142        0  \n",
              "48  0.229575  0.292986  0.297331  0.204221  0.205462  2.142        1  \n",
              "\n",
              "[220 rows x 189 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### nan value\n",
        "LF_train = LF_train.fillna(0)\n",
        "LF_train = LF_train.replace('Not able to trot', 0)\n",
        "LF_train['Speed'] = LF_train['Speed'].astype(float)\n",
        "\n",
        "for col in LF_train.columns:\n",
        "    if 'V' in col or 'Speed' in col :\n",
        "        LF_train[col] = LF_train[col].replace(0, LF_train[col].mean())\n",
        "\n",
        "### sort by id\n",
        "LF_train = LF_train.sort_values(by=['id'])\n",
        "\n",
        "### perform PCA and get rid of redundant columns\n",
        "\n",
        "LF_train.to_csv('LF_train.csv')\n",
        "LF_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exUXE_T4c4DV"
      },
      "source": [
        "**FFNN construct**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "r0PjZPW60Y5W"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/69/r1dg6qsd2_j9csr9658538qw0000gn/T/ipykernel_62573/1063272524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Setting seed ***DO NOT MODIFY***\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# Lambda to switch to GPU if available\n",
        "get_device = lambda : \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "\n",
        "################################################################################\n",
        "#########################             ADDED             ########################\n",
        "################################################################################\n",
        "def weight_init(m):\n",
        "\tif isinstance(m, nn.Linear):\n",
        "\t\tnn.init.xavier_uniform_(m.weight)\n",
        "\t\tnn.init.constant_(m.bias, 0.)\n",
        "\n",
        "# Consult the PyTorch documentation for information on the functions used below:\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "\tdef __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size):\n",
        "\t\tsuper(FFNN, self).__init__()\n",
        "\t\t############################################################################\n",
        "\t\t#########################             ADDED             ####################\n",
        "\t\t############################################################################\n",
        "\t\tself.loss_class_weights = torch.tensor([0.5, 1, 1, 1, 1, 1, 1, 1, 1], \n",
        "\t\t                                       dtype=torch.float)\n",
        "\t\tself.embedding = nn.Embedding(vocab_size, embedding_dim, max_norm=True)\n",
        "\t  ### TODO : initialize your model with the necessary layers and functions ###\n",
        "\t\t\n",
        "\n",
        "\t\t### Here are pytorch docs which you may find useful:\n",
        "\t\t### Linear layer:\n",
        "\t\t###\t\thttps://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "\t\tself.W = nn.Linear(embedding_dim, hidden_dim)\n",
        "\t\tself.W_x = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\t\t### ReLU: \n",
        "\t\t###\t\thttps://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
        "\t\tself.relu = nn.ReLU()           \n",
        "\t\t### LogSoftmax:\n",
        "\t\t###\t\thttps://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html\n",
        "\t\tself.softmax = nn.LogSoftmax(dim = 1)\n",
        "\t\t### NLLoss:\n",
        "\t\t###\t\thttps://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
        "\t\tself.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\t##############################################################################\n",
        "\t#########################           CHANGED             ######################\n",
        "\t##############################################################################\n",
        "\tdef compute_Loss(self, predicted_vector, gold_label, masks):\n",
        "\t\treturn self.loss(predicted_vector[masks], gold_label[masks])\n",
        "\n",
        "\tdef forward(self, input_vector):\n",
        "\t\t############################################################################\n",
        "\t\t#########################             ADDED             ####################\n",
        "\t\t############################################################################\n",
        "\t\t # input_vector=(batch_size, max_len)\n",
        "\t\toriginal_shape = input_vector.shape\n",
        "\n",
        "\t\tinput_vector = input_vector.reshape(-1)\n",
        "\n",
        "\t\t\n",
        "\t\t# The z_i are just there to record intermediary computations for your clarity\n",
        "\t\tembeddings = self.embedding(input_vector) \n",
        "\t\tz1 = self.W(embeddings)\n",
        "\t\t\n",
        "\t\t# correction 1: No activation on z1; no linear layer for z2; mistakingly softmax z1\n",
        "\t\tz1_relu = self.relu(z1)\n",
        "\t\tz2 = self.W_x(z1_relu)\n",
        "\t\tpredicted_vector = self.softmax(z2)\n",
        "\t\t# predicted_vector = self.softmax(z1)\n",
        "\t\t# correction 1 end\n",
        "\t\t\n",
        "\t\t# predicted_vector=(batch_size, max_len, output_dim)\n",
        "\t\tpredicted_vector = predicted_vector.reshape((original_shape[0],\n",
        "                                                 original_shape[1], -1))\n",
        "\n",
        "\t\treturn predicted_vector\n",
        "\n",
        "\tdef load_model(self, save_path):\n",
        "\t\tself.load_state_dict(torch.load(save_path))\n",
        "\n",
        "\tdef save_model(self, save_path):\n",
        "\t\ttorch.save(self.state_dict(), save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWwIlNyLdMPR"
      },
      "source": [
        "**FFNN training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twunIbprc1ow"
      },
      "outputs": [],
      "source": [
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "################################################################################\n",
        "#########################             CHANGED             ######################\n",
        "################################################################################\n",
        "def train_epoch(model, train_loader, optimizer):\n",
        "  model.train()\n",
        "  total = 0\n",
        "  batch = 0\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "  for (input_batch, expected_out, batch_mask) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n",
        "    optimizer.zero_grad()\n",
        "    # correction 2: batch = 1 is incorrect\n",
        "    batch += 1\n",
        "    flattened_expected_out = expected_out.reshape(-1).to(device)\n",
        "    flattened_batch_mask = batch_mask.reshape(-1).to(device)\n",
        "    output = model(input_batch.to(get_device())).to(get_device())\n",
        "    flattened_output = output.reshape(-1, output.shape[-1])\n",
        "    loss = model.compute_Loss(flattened_output, flattened_expected_out, flattened_batch_mask)\n",
        "    total += batch_mask.sum().item()\n",
        "    _, predicted = torch.max(output, -1)\n",
        "    flattened_predicted = predicted.reshape(-1)\n",
        "    # correction 3: We think correct should increase instead of decrease\n",
        "    # correct -= (flattened_expected_out[flattened_batch_mask].to(\"cpu\") == flattened_predicted[flattened_batch_mask].to(\"cpu\")).cpu().numpy().sum()\n",
        "    correct += (flattened_expected_out[flattened_batch_mask].to(\"cpu\") == flattened_predicted[flattened_batch_mask].to(\"cpu\")).cpu().numpy().sum()\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    # correction 4: SGD wasn't performed\n",
        "    optimizer.step()\n",
        "    \n",
        "  print(\"Loss: \" + str(total_loss/batch))\n",
        "  print(\"Training Accuracy: \" + str(correct/total))\n",
        "  return total_loss/batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA1zCWZV1PnL"
      },
      "outputs": [],
      "source": [
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "\n",
        "def evaluation(model, val_loader, optimizer):\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for (input_batch, expected_out) in tqdm(val_loader, leave=False, desc=\"Validation Batches\"):\n",
        "    output = model.forward(input_batch.to(get_device())).to(get_device())\n",
        "    total += output.size()[1]\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    correct += (expected_out.to(\"cpu\") == predicted.to(\"cpu\")).cpu().numpy().sum()\n",
        "    loss += model.compute_Loss(output, expected_out.to(get_device()))\n",
        "  loss /= len(val_loader)\n",
        "  print(\"Validation Loss: \" + str(loss.item()))\n",
        "  print(\"Validation Accuracy: \" + str(correct/total))\n",
        "  print()\n",
        "  return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSsHRUXReLPn"
      },
      "outputs": [],
      "source": [
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "def train_and_evaluate(number_of_epochs, model, train_loader, val_loader, min_loss=0, lr=.001):\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=.01)\n",
        "  loss_values = [[],[]]\n",
        "  for epoch in trange(number_of_epochs, desc=\"Epochs\"):\n",
        "    cur_loss = train_epoch(model, train_loader, optimizer)\n",
        "    loss_values[0].append(cur_loss)\n",
        "    cur_loss_val = evaluation(model, val_loader, optimizer)\n",
        "    loss_values[1].append(cur_loss_val)\n",
        "    if cur_loss <= min_loss: return loss_values\n",
        "  return loss_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "TCa0WxPl1Wbl",
        "outputId": "2d8bc4e0-70f0-41ee-a4f9-a79f7f330097"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-f5d7a3ffa4b9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m### TODO: train and evaluate the model with the functions and data above ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresult_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_and_evaluate' is not defined"
          ]
        }
      ],
      "source": [
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "\n",
        "### TODO: add code for creating model (check updated header for FFNN)\n",
        "model = FFNN(300, 150, 9, 12414)\n",
        "\n",
        "### Initialize model weights\n",
        "model.apply(weight_init)\n",
        "\n",
        "### TODO: train and evaluate the model with the functions and data above ###\n",
        "result_model = train_and_evaluate(4, model.cuda(), train_loader, val_loader, 0.2, 0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aob-sNRxMVA3"
      },
      "outputs": [],
      "source": [
        "# TODO : add a single line code that saves your model in order to prevent re-training the model for later use.\n",
        "\n",
        "model.save_model(\"ffnn_kaggle.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P01jb2711YO-"
      },
      "outputs": [],
      "source": [
        "# Example of how to load\n",
        "ffnn = FFNN(300, 150, 9, 12414)\n",
        "ffnn.load_model(\"ffnn_kaggle.pth\")\n",
        "ffnn = ffnn.to(get_device())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7QbF2FVdSvQ"
      },
      "source": [
        "**Single hidden Layer RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myBb48Kmfs0k"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "#########################             CHANGED             ######################\n",
        "################################################################################\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, max_norm=True)\n",
        "        self.loss_class_weights = torch.tensor([0.5, 1, 1, 1, 1, 1, 1, 1, 1], \n",
        "                                               dtype=torch.float)\n",
        "        ### TODO : Initialize dimensions of all layers.\n",
        "        ### TODO : Initialize three linear layers:\n",
        "              # 1. An input layer\n",
        "        self.input_layer=nn.Linear(embedding_dim, hidden_dim)\n",
        "              # 2. A hidden layer\n",
        "        self.hidden_layer=nn.Linear(hidden_dim,hidden_dim)\n",
        "              # 3. An output layer\n",
        "        self.output_layer=nn.Linear(hidden_dim,output_dim)\n",
        "        ### TODO : Initialize the activation function.\n",
        "        self.relu=nn.ReLU()\n",
        "        ### TODO : Initialize softmax and loss functions.\n",
        "        self.lsmax=nn.LogSoftmax(dim = -1)\n",
        "        self.loss=nn.CrossEntropyLoss()\n",
        "\n",
        "        self.h_layer=torch.zeros(1, hidden_dim,dtype=torch.float, device = torch.device(\"cuda:0\"))\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label, masks):\n",
        "        return self.loss(predicted_vector[masks], gold_label[masks])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ### TODO : Write the forward function such that it processes the sentences incrementally. \n",
        "        ### TODO : Return output of the softmax across all time steps\n",
        "\n",
        "        original_shape = inputs.shape\n",
        "        # not sure about this\n",
        "        h_i = torch.zeros_like(self.h_layer)\n",
        "        y = torch.Tensor([]).to(get_device())\n",
        "        # t is time_step\n",
        "        for t in range(max_len):\n",
        "          embeddings = self.embedding(inputs[:, t])\n",
        "          h_i =self.relu(self.hidden_layer(h_i) + self.input_layer(embeddings))\n",
        "          y_i = self.lsmax(self.output_layer(h_i))\n",
        "          shape_y_i = y_i.shape\n",
        "          y_i = y_i.reshape((-1, shape_y_i[0], shape_y_i[1]))\n",
        "          # print(y_i.shape)\n",
        "          y = torch.cat((y, y_i), -1)\n",
        "          \n",
        "\n",
        "        output = y.reshape((original_shape[0],\n",
        "                                                 original_shape[1], -1))\n",
        "        return output\n",
        "\n",
        "    def load_model(self, save_path):\n",
        "        self.load_state_dict(torch.load(save_path))\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        torch.save(self.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQxtQ66Kf0oz"
      },
      "outputs": [],
      "source": [
        "### TODO: add code for creating model (check updated header for RNN)\n",
        "rnn = RNN(150, 100, 9, 12414)\n",
        "\n",
        "### Initialize model weights\n",
        "rnn.apply(weight_init)\n",
        "\n",
        "### TODO: train and evaluate the model with the functions and data above ###\n",
        "\n",
        "result_model = train_and_evaluate(6, rnn.cuda(), train_loader, val_loader, 0.05, 0.001)\n",
        "\n",
        "# print(\"I'm not completed yet!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSPTGS2zf-Zo"
      },
      "outputs": [],
      "source": [
        "rnn.save_model('rnn_kaggle.pth')\n",
        "rnn.load_model('rnn_kaggle.pth')\n",
        "rnn = rnn.to(get_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SMVOO0UgJlT"
      },
      "outputs": [],
      "source": [
        "class RNN2(nn.Module):\n",
        "    ### TODO : Modify __init__ header ###\n",
        "    # sl: I think the _init_ header is already initialized, so just left it as it is \n",
        "    ### TODO : Initialize n hidden linear layers in your __init__ ###\n",
        "    # sl: modified the h_layer attribute to be a list of hidden layers instead of a single layer\n",
        "    ### TODO : Modify your forward header ###\n",
        "    # sl: I think the header is already modified when given, not so sure what other arguments we will need\n",
        "    ### TODO : Modify your forward function to: ###\n",
        "        # 1. Pass the data through each hidden layer #\n",
        "        # 2. Save the activation of each layer at each timestep when training=FALSE #\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size, hidden_layers = 1): \n",
        "        super(RNN2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, max_norm=True)\n",
        "        self.loss_class_weights = torch.tensor([0.5, 1, 1, 1, 1, 1, 1, 1, 1], \n",
        "                                               dtype=torch.float)\n",
        "              # 1. An input layer\n",
        "        self.input_layer=nn.Linear(embedding_dim, hidden_dim)\n",
        "              # 2. hidden_layers hidden layer\n",
        "        self.hidden_layer=nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(hidden_layers)])\n",
        "              # 3. An output layer\n",
        "        self.output_layer=nn.Linear(hidden_dim,output_dim)\n",
        "        ### TODO : Initialize the activation function.\n",
        "        self.relu=nn.ReLU()\n",
        "        ### TODO : Initialize softmax and loss functions.\n",
        "        self.lsmax=nn.LogSoftmax(dim = 1)\n",
        "        self.loss=nn.CrossEntropyLoss()\n",
        "        ### sl: initiate a list to save the activations according to post #524\n",
        "        self.save_act = []\n",
        "        self.hidden_dim = hidden_dim\n",
        "        ### 10/25\n",
        "        self.hid2hid = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(hidden_layers-1)])\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label, masks):\n",
        "        return self.loss(predicted_vector[masks], gold_label[masks])\n",
        "\n",
        "    def forward(self, inputs, training = True):\n",
        "        ### TODO : Write the forward function such that it processes the sentences incrementally. \n",
        "        ### TODO : Return output of the softmax across all time steps\n",
        "        ### sl: modified 10/22: nn does NOT step given a python list. It only optimize\n",
        "        \n",
        "        h_i = [torch.zeros(1, self.hidden_dim,dtype=torch.float, device = torch.device(\"cuda:0\")) for _ in range(len(self.hidden_layer))]\n",
        "        output = torch.Tensor([]).to(get_device())\n",
        "        original_shape = inputs.shape\n",
        "        # go through each token\n",
        "        for t in range(max_len):\n",
        "          # copied from FFNN\n",
        "          embeddings = self.embedding(inputs[:, t])\n",
        "          for count, hidden in enumerate(self.hidden_layer):\n",
        "            if count == 0: \n",
        "              h_i[count] = self.relu(hidden(h_i[count]) + self.input_layer(embeddings))\n",
        "            else:\n",
        "              h_i[count] = self.relu(hidden(h_i[count]) + self.hid2hid[count - 1](h_i[count - 1]))\n",
        "            # if we are testing, we should save the activation results, which is - \n",
        "            if training == False:\n",
        "              self.save_act.append(h_i[count])\n",
        "          # end for loop\n",
        "          output_i = self.lsmax(self.output_layer(h_i[-1]))\n",
        "          self.save_act.append(output_i)\n",
        "          shape_output_i = output_i.shape\n",
        "          output_i = output_i.reshape((-1, shape_output_i[0], shape_output_i[1]))\n",
        "          output = torch.cat((output, output_i), -1)\n",
        "          \n",
        "        output = output.reshape((original_shape[0], original_shape[1], -1))\n",
        "        return output\n",
        "\n",
        "\n",
        "    def load_model(self, save_path):\n",
        "        self.load_state_dict(torch.load(save_path))\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        torch.save(self.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bvzv8FegOhL"
      },
      "outputs": [],
      "source": [
        "### TODO : Train and evaluate your RNN2 ###\n",
        "rnn_2 = RNN2(150, 192, 9, 12414, 3)\n",
        "\n",
        "### Initialize model weights\n",
        "rnn_2.apply(weight_init)\n",
        "\n",
        "### TODO: train and evaluate the model with the functions and data above ###\n",
        "\n",
        "result_model = train_and_evaluate(2, rnn_2.cuda(), train_loader, val_loader, 0.2, 0.001)\n",
        "\n",
        "# print(\"I'm not completed yet!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwRO1FTVgRMg"
      },
      "outputs": [],
      "source": [
        "rnn_2.save_model('rnn_2_kaggle.pth')\n",
        "rnn_2 = RNN2(150, 192, 9, 12414, 3)\n",
        "rnn_2.load_model('rnn_2_kaggle.pth')\n",
        "rnn_2 = rnn_2.to(get_device())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJlT6p4FgdjR"
      },
      "source": [
        "**create submission**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUc9Z8ECgdHr"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from pyparsing.helpers import TokenConverter\n",
        "### TODO : pass the processed test data through the model ###\n",
        "\n",
        "with torch.no_grad():\n",
        "  # print(\"I'm not completed yet!\")\n",
        "  ffnn.eval()\n",
        "  count = 0\n",
        "  flattened_predicted_all = torch.Tensor([]).to(get_device())\n",
        "  for (input_batch, expected_out, batch_mask) in tqdm(test_loader, leave=False, desc=\"Validation Batches\"):\n",
        "    # next line commented out since there is no expected output\n",
        "    # flattened_expected_out = expected_out.reshape(-1).to(device)\n",
        "    flattened_batch_mask = batch_mask.reshape(-1).to(device)\n",
        "    output = rnn(input_batch.to(get_device())).to(get_device())\n",
        "    # next line seems not involved in the outputs so commented out\n",
        "    # flattened_output = output.reshape(-1, output.shape[-1])\n",
        "    _, predicted = torch.max(output, -1)\n",
        "    flattened_predicted = predicted.reshape(-1)\n",
        "    flattened_predicted_all = torch.concat((flattened_predicted_all, flattened_predicted))\n",
        "  # mask\n",
        "  flattened_mask = list(itertools.chain.from_iterable(processed_test['mask']))\n",
        "  # count is the index, predicted is the prediction for each token\n",
        "  real_predicted = torch.Tensor([])\n",
        "  for count, predicted in enumerate(flattened_predicted_all):\n",
        "    if flattened_mask[count] == 1:\n",
        "      predicted = torch.Tensor([predicted])\n",
        "      real_predicted = torch.concat((real_predicted, predicted))\n",
        "  real_predicted = real_predicted.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXt7ZsGRguCo"
      },
      "outputs": [],
      "source": [
        "### TODO : extract labels and indices for model predictions of named entities ###\n",
        "\n",
        "# Done\n",
        "indices = list(chain.from_iterable(test['index']))\n",
        "num2tag = {y: x for x, y in category_map.items()}\n",
        "res = []\n",
        "for num in real_predicted:\n",
        "  res.append(num2tag[num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHuT-5VENZIN"
      },
      "outputs": [],
      "source": [
        "def format_output_labels(token_labels, token_indices):\n",
        "    \"\"\"\n",
        "    Returns a dictionary that has the labels (LOC, ORG, MISC or PER) as the keys, \n",
        "    with the associated value being the list of entities predicted to be of that key label. \n",
        "    Each entity is specified by its starting and ending position indicated in [token_indices].\n",
        "\n",
        "    Eg. if [token_labels] = [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"B-ORG\"]\n",
        "           [token_indices] = [15, 16, 17, 18, 19]\n",
        "        then dictionary returned is \n",
        "        {'LOC': [], 'MISC': [], 'ORG': [(15, 16), (19, 19)], 'PER': []}\n",
        "\n",
        "    :parameter token_labels: A list of token labels (eg. B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, B-MISC, OR I-MISC).\n",
        "    :type token_labels: List[String]\n",
        "    :parameter token_indices: A list of token indices (taken from the dataset) \n",
        "                              corresponding to the labels in [token_labels].\n",
        "    :type token_indices: List[int]\n",
        "    \"\"\"\n",
        "    label_dict = {\"LOC\":[], \"MISC\":[], \"ORG\":[], \"PER\":[]}\n",
        "    prev_label = 'O'\n",
        "    start = token_indices[0]\n",
        "    for idx, label in enumerate(token_labels):\n",
        "      curr_label = label.split('-')[-1]\n",
        "      if label.startswith('B-') or curr_label != prev_label:\n",
        "        if prev_label != 'O':\n",
        "          label_dict[prev_label].append((start, token_indices[idx-1]))\n",
        "        if curr_label != 'O':\n",
        "          start = token_indices[idx]\n",
        "        else:\n",
        "          start = None\n",
        "      \n",
        "      prev_label = curr_label\n",
        "\n",
        "    if start is not None and prev_label != 'O':\n",
        "      label_dict[prev_label].append((start, token_indices[idx]))\n",
        "    return label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-XrcS8Z0BSk"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def create_submission(output_filepath, token_labels, token_inds):\n",
        "    \"\"\"\n",
        "    :parameter output_filepath: The full path (including file name) of the output file, \n",
        "                                with extension .csv\n",
        "    :type output_filepath: [String]\n",
        "    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n",
        "    :type token_labels: List[String]\n",
        "    :parameter token_indices: A list of token indices (taken from the dataset) \n",
        "                              corresponding to the labels in [token_labels].\n",
        "    :type token_indices: List[int]\n",
        "    \"\"\"\n",
        "    label_dict = format_output_labels(token_labels, token_inds)\n",
        "    with open(output_filepath, mode='w') as csv_file:\n",
        "        fieldnames = ['Id', 'Predicted']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for key in label_dict:\n",
        "            p_string = \" \".join([str(start)+\"-\"+str(end) for start,end in label_dict[key]])\n",
        "            writer.writerow({'Id': key, 'Predicted': p_string})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCmEcvWK05Yx"
      },
      "outputs": [],
      "source": [
        "create_submission('drive/MyDrive/Colab Notebooks/rnn_kaggle.csv', res, indices)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
