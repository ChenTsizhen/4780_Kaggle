{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is for kaggle competition"
      ],
      "metadata": {
        "id": "ll-B2gfPHc75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tons and tons of imports!\n",
        "from collections import Counter, namedtuple\n",
        "from itertools import chain\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
      ],
      "metadata": {
        "id": "FJU9aA6rJ_ww"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "id": "Hkn9JgTYH9IK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf15d21-63e9-4455-b74d-fe61c8ad14f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "unDkKa--dGWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LF_train  = pd.read_csv('drive/MyDrive/Colab Notebooks/kaggle_ml/data/LF_train.csv')\n",
        "LF_test = pd.read_csv('drive/MyDrive/Colab Notebooks/kaggle_ml/data/LF_test.csv')\n",
        "LH_train = pd.read_csv('drive/MyDrive/Colab Notebooks/kaggle_ml/data/LH_train.csv')\n",
        "LH_test = pd.read_csv('drive/MyDrive/Colab Notebooks/kaggle_ml/data/LH_test.csv')\n",
        "RF_train  = pd.read_csv('drive/MyDrive/Colab Notebooks/kaggle_ml/data/RF_train.csv')\n",
        "RF_test = pd.read_csv('drive/MyDrive/Colab Notebooks/kaggle_ml/data/RF_test.csv')\n",
        "RH_train = pd.read_csv('drive/MyDrive/Colab Notebooks/kaggle_ml/data/RH_train.csv')\n",
        "RH_test = pd.read_csv('drive/MyDrive/Colab Notebooks/kaggle_ml/data/RH_test.csv')"
      ],
      "metadata": {
        "id": "mQWFCYM3Lw-h"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### turn literals into numericals\n",
        "LF_train = LF_train.drop(columns=['dob', 'gait', 'Gait'])\n",
        "LF_train['forceplate_date'] = LF_train['forceplate_date'].str.replace(\"-\",\"\").astype(int)\n",
        "\n",
        "# split by walk / trot\n",
        "trot_list = []\n",
        "name_list = []\n",
        "for col in LF_train.columns:\n",
        "    if 'trot' in col:\n",
        "        trot_list.append(col)\n",
        "\n",
        "        new_name = col.split('_trot')\n",
        "        name_list.append(new_name[0])\n",
        "\n",
        "LF_train_trot = pd.DataFrame(LF_train, columns = trot_list)\n",
        "LF_train_trot.columns = name_list\n",
        "\n",
        "both = ['id', 'gender', 'weight', 'forceplate_date', 'speed', 'age', 'Speed', 'LF']\n",
        "LF_train_both = pd.DataFrame(LF_train, columns = both)\n",
        "\n",
        "# add binary col is_trot\n",
        "LF_train_trot = LF_train_both.join(LF_train_trot)\n",
        "LF_train_trot['is_trot'] = 1\n",
        "\n",
        "LF_train_walk = LF_train.drop(columns=trot_list)\n",
        "LF_train_walk['is_trot'] = 0\n",
        "\n",
        "# concat walk and trot\n",
        "LF_train = pd.concat([LF_train_walk, LF_train_trot])\n",
        "LF_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "AhGRrh2L5PNI",
        "outputId": "2290d667-29fc-4f40-99ee-7f88e5af41b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id  gender  weight  forceplate_date       age  LF               speed  \\\n",
              "0    169       0    60.0         20090814  1.542466   0              0.9925   \n",
              "1     35       0    37.0         20110719  5.271233   0               1.046   \n",
              "2    166       1    24.1         20080430  8.605479   0               1.152   \n",
              "3     41       0    69.3         20100622  7.304110   0                0.95   \n",
              "4     99       1    26.7         20090123  6.580822   0               1.014   \n",
              "..   ...     ...     ...              ...       ...  ..                 ...   \n",
              "105   26       0    40.6         20090707  3.484932   0  1.0020000000000002   \n",
              "106   10       0    31.1         20150622  8.673973   0                1.38   \n",
              "107  154       1    23.9         20080513  2.243836   0  1.1775000000000002   \n",
              "108   20       0    26.4         20090821  7.301370   0               1.162   \n",
              "109   79       1    31.7         20111201  4.693151   0  1.1233333333333333   \n",
              "\n",
              "          V1_LF       V1_RF       V1_LH  ...    V29_LF    V29_RF    V29_LH  \\\n",
              "0    381.595253  379.793688  240.016723  ...  0.290163  0.318380  0.185754   \n",
              "1    241.735368  238.213439  133.789192  ...  0.398267  0.396805  0.087256   \n",
              "2    183.448346  180.232043  100.783343  ...  0.399387  0.342216  0.133397   \n",
              "3    457.200158  446.476056  271.393615  ...  0.255895  0.275767  0.231840   \n",
              "4    174.265686  172.449775  103.717135  ...  0.372091  0.341302  0.055101   \n",
              "..          ...         ...         ...  ...       ...       ...       ...   \n",
              "105  383.750060  412.596446  170.448357  ...  0.285190  0.354662  0.161317   \n",
              "106         NaN         NaN         NaN  ...       NaN       NaN       NaN   \n",
              "107  252.723577  257.584865  153.550887  ...  0.279639  0.311866  0.214374   \n",
              "108  324.181908  325.893424  217.043072  ...  0.295705  0.305411  0.247534   \n",
              "109  310.539920  319.703120  207.135704  ...  0.313494  0.287276  0.223702   \n",
              "\n",
              "       V29_RH    V30_LF    V30_RF    V30_LH    V30_RH               Speed  \\\n",
              "0    0.205703  0.255894  0.251542  0.246282  0.246282                 NaN   \n",
              "1    0.117672  0.259196  0.261323  0.229215  0.250266  1.9274999999999998   \n",
              "2    0.124999  0.257310  0.264327  0.235965  0.242398               2.045   \n",
              "3    0.236498  0.251910  0.256917  0.250856  0.240316                2.26   \n",
              "4    0.231507  0.261352  0.259839  0.245964  0.232846  1.8933333333333335   \n",
              "..        ...       ...       ...       ...       ...                 ...   \n",
              "105  0.198831  0.256332  0.272036  0.214792  0.256839              2.0025   \n",
              "106       NaN       NaN       NaN       NaN       NaN                 NaN   \n",
              "107  0.194122  0.271323  0.275353  0.218267  0.235057  1.8824999999999998   \n",
              "108  0.151350  0.275362  0.277253  0.221802  0.225583               2.182   \n",
              "109  0.175527  0.278817  0.280024  0.232348  0.208811               1.785   \n",
              "\n",
              "     is_trot  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  \n",
              "..       ...  \n",
              "105        1  \n",
              "106        1  \n",
              "107        1  \n",
              "108        1  \n",
              "109        1  \n",
              "\n",
              "[220 rows x 189 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8516ffa7-38ce-40df-86ed-636bcc151e81\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>weight</th>\n",
              "      <th>forceplate_date</th>\n",
              "      <th>age</th>\n",
              "      <th>LF</th>\n",
              "      <th>speed</th>\n",
              "      <th>V1_LF</th>\n",
              "      <th>V1_RF</th>\n",
              "      <th>V1_LH</th>\n",
              "      <th>...</th>\n",
              "      <th>V29_LF</th>\n",
              "      <th>V29_RF</th>\n",
              "      <th>V29_LH</th>\n",
              "      <th>V29_RH</th>\n",
              "      <th>V30_LF</th>\n",
              "      <th>V30_RF</th>\n",
              "      <th>V30_LH</th>\n",
              "      <th>V30_RH</th>\n",
              "      <th>Speed</th>\n",
              "      <th>is_trot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>169</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>20090814</td>\n",
              "      <td>1.542466</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9925</td>\n",
              "      <td>381.595253</td>\n",
              "      <td>379.793688</td>\n",
              "      <td>240.016723</td>\n",
              "      <td>...</td>\n",
              "      <td>0.290163</td>\n",
              "      <td>0.318380</td>\n",
              "      <td>0.185754</td>\n",
              "      <td>0.205703</td>\n",
              "      <td>0.255894</td>\n",
              "      <td>0.251542</td>\n",
              "      <td>0.246282</td>\n",
              "      <td>0.246282</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>20110719</td>\n",
              "      <td>5.271233</td>\n",
              "      <td>0</td>\n",
              "      <td>1.046</td>\n",
              "      <td>241.735368</td>\n",
              "      <td>238.213439</td>\n",
              "      <td>133.789192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.398267</td>\n",
              "      <td>0.396805</td>\n",
              "      <td>0.087256</td>\n",
              "      <td>0.117672</td>\n",
              "      <td>0.259196</td>\n",
              "      <td>0.261323</td>\n",
              "      <td>0.229215</td>\n",
              "      <td>0.250266</td>\n",
              "      <td>1.9274999999999998</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>166</td>\n",
              "      <td>1</td>\n",
              "      <td>24.1</td>\n",
              "      <td>20080430</td>\n",
              "      <td>8.605479</td>\n",
              "      <td>0</td>\n",
              "      <td>1.152</td>\n",
              "      <td>183.448346</td>\n",
              "      <td>180.232043</td>\n",
              "      <td>100.783343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.399387</td>\n",
              "      <td>0.342216</td>\n",
              "      <td>0.133397</td>\n",
              "      <td>0.124999</td>\n",
              "      <td>0.257310</td>\n",
              "      <td>0.264327</td>\n",
              "      <td>0.235965</td>\n",
              "      <td>0.242398</td>\n",
              "      <td>2.045</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>69.3</td>\n",
              "      <td>20100622</td>\n",
              "      <td>7.304110</td>\n",
              "      <td>0</td>\n",
              "      <td>0.95</td>\n",
              "      <td>457.200158</td>\n",
              "      <td>446.476056</td>\n",
              "      <td>271.393615</td>\n",
              "      <td>...</td>\n",
              "      <td>0.255895</td>\n",
              "      <td>0.275767</td>\n",
              "      <td>0.231840</td>\n",
              "      <td>0.236498</td>\n",
              "      <td>0.251910</td>\n",
              "      <td>0.256917</td>\n",
              "      <td>0.250856</td>\n",
              "      <td>0.240316</td>\n",
              "      <td>2.26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>99</td>\n",
              "      <td>1</td>\n",
              "      <td>26.7</td>\n",
              "      <td>20090123</td>\n",
              "      <td>6.580822</td>\n",
              "      <td>0</td>\n",
              "      <td>1.014</td>\n",
              "      <td>174.265686</td>\n",
              "      <td>172.449775</td>\n",
              "      <td>103.717135</td>\n",
              "      <td>...</td>\n",
              "      <td>0.372091</td>\n",
              "      <td>0.341302</td>\n",
              "      <td>0.055101</td>\n",
              "      <td>0.231507</td>\n",
              "      <td>0.261352</td>\n",
              "      <td>0.259839</td>\n",
              "      <td>0.245964</td>\n",
              "      <td>0.232846</td>\n",
              "      <td>1.8933333333333335</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>40.6</td>\n",
              "      <td>20090707</td>\n",
              "      <td>3.484932</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0020000000000002</td>\n",
              "      <td>383.750060</td>\n",
              "      <td>412.596446</td>\n",
              "      <td>170.448357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285190</td>\n",
              "      <td>0.354662</td>\n",
              "      <td>0.161317</td>\n",
              "      <td>0.198831</td>\n",
              "      <td>0.256332</td>\n",
              "      <td>0.272036</td>\n",
              "      <td>0.214792</td>\n",
              "      <td>0.256839</td>\n",
              "      <td>2.0025</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>31.1</td>\n",
              "      <td>20150622</td>\n",
              "      <td>8.673973</td>\n",
              "      <td>0</td>\n",
              "      <td>1.38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>154</td>\n",
              "      <td>1</td>\n",
              "      <td>23.9</td>\n",
              "      <td>20080513</td>\n",
              "      <td>2.243836</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1775000000000002</td>\n",
              "      <td>252.723577</td>\n",
              "      <td>257.584865</td>\n",
              "      <td>153.550887</td>\n",
              "      <td>...</td>\n",
              "      <td>0.279639</td>\n",
              "      <td>0.311866</td>\n",
              "      <td>0.214374</td>\n",
              "      <td>0.194122</td>\n",
              "      <td>0.271323</td>\n",
              "      <td>0.275353</td>\n",
              "      <td>0.218267</td>\n",
              "      <td>0.235057</td>\n",
              "      <td>1.8824999999999998</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>26.4</td>\n",
              "      <td>20090821</td>\n",
              "      <td>7.301370</td>\n",
              "      <td>0</td>\n",
              "      <td>1.162</td>\n",
              "      <td>324.181908</td>\n",
              "      <td>325.893424</td>\n",
              "      <td>217.043072</td>\n",
              "      <td>...</td>\n",
              "      <td>0.295705</td>\n",
              "      <td>0.305411</td>\n",
              "      <td>0.247534</td>\n",
              "      <td>0.151350</td>\n",
              "      <td>0.275362</td>\n",
              "      <td>0.277253</td>\n",
              "      <td>0.221802</td>\n",
              "      <td>0.225583</td>\n",
              "      <td>2.182</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>79</td>\n",
              "      <td>1</td>\n",
              "      <td>31.7</td>\n",
              "      <td>20111201</td>\n",
              "      <td>4.693151</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1233333333333333</td>\n",
              "      <td>310.539920</td>\n",
              "      <td>319.703120</td>\n",
              "      <td>207.135704</td>\n",
              "      <td>...</td>\n",
              "      <td>0.313494</td>\n",
              "      <td>0.287276</td>\n",
              "      <td>0.223702</td>\n",
              "      <td>0.175527</td>\n",
              "      <td>0.278817</td>\n",
              "      <td>0.280024</td>\n",
              "      <td>0.232348</td>\n",
              "      <td>0.208811</td>\n",
              "      <td>1.785</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220 rows × 189 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8516ffa7-38ce-40df-86ed-636bcc151e81')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8516ffa7-38ce-40df-86ed-636bcc151e81 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8516ffa7-38ce-40df-86ed-636bcc151e81');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### nan value\n",
        "LF_train = LF_train.fillna(0)\n",
        "LF_train = LF_train.replace('Not able to trot', 0)\n",
        "LF_train['Speed'] = LF_train['Speed'].astype(float)\n",
        "\n",
        "for col in LF_train.columns:\n",
        "    if 'V' in col or 'Speed' in col :\n",
        "        LF_train[col] = LF_train[col].replace(0, LF_train[col].mean())\n",
        "\n",
        "### sort by id\n",
        "LF_train = LF_train.sort_values(by=['id'])\n",
        "\n",
        "### perform PCA and get rid of redundant columns\n",
        "\n",
        "LF_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "rkl-LSqz5l7u",
        "outputId": "886be2f5-3ef5-41d1-e5fd-60f507baf235"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  gender  weight  forceplate_date       age  LF               speed  \\\n",
              "67    2       0    34.6         20190626  5.079452   1               1.315   \n",
              "67    2       0    34.6         20190626  5.079452   1               1.315   \n",
              "76    3       1    25.6         20191003  7.178082   0  1.1400000000000001   \n",
              "76    3       1    25.6         20191003  7.178082   0  1.1400000000000001   \n",
              "94    6       0    36.7         20110425  1.772603   1               1.032   \n",
              "..  ...     ...     ...              ...       ...  ..                 ...   \n",
              "92  180       1    26.3         20080609  3.942466   0  1.0779999999999998   \n",
              "53  181       1    20.2         20090226  8.986301   0               1.188   \n",
              "53  181       1    20.2         20090226  8.986301   0               1.188   \n",
              "48  184       0    22.6         20090128  1.890411   0  1.1883333333333332   \n",
              "48  184       0    22.6         20090128  1.890411   0  1.1883333333333332   \n",
              "\n",
              "         V1_LF       V1_RF       V1_LH  ...    V29_LF    V29_RF    V29_LH  \\\n",
              "67  183.831519  261.735816  173.086274  ...  0.354105  0.485929  0.075465   \n",
              "67  185.208712  249.535675  145.922523  ...  0.378632  0.375222  0.136255   \n",
              "76  280.954561  290.108186  144.419058  ...  0.283679  0.339011  0.198281   \n",
              "76  178.404307  194.485630  115.977757  ...  0.402185  0.492270  0.076547   \n",
              "94  248.316618  245.881475  150.944727  ...  0.303288  0.328469  0.177950   \n",
              "..         ...         ...         ...  ...       ...       ...       ...   \n",
              "92  169.406911  172.968121  110.135375  ...  0.281286  0.276961  0.196854   \n",
              "53  211.831023  212.581617  131.925549  ...  0.303722  0.298653  0.204484   \n",
              "53  128.704239  127.564176   81.999105  ...  0.346563  0.344794  0.157216   \n",
              "48  157.895443  156.617448  102.073996  ...  0.424436  0.396303  0.109802   \n",
              "48  249.485374  250.633042  161.854799  ...  0.261071  0.261597  0.247756   \n",
              "\n",
              "      V29_RH    V30_LF    V30_RF    V30_LH    V30_RH  Speed  is_trot  \n",
              "67  0.084501  0.235294  0.249135  0.262976  0.252595  1.710        0  \n",
              "67  0.109891  0.250000  0.272321  0.232143  0.245536  1.710        1  \n",
              "76  0.179028  0.291317  0.282913  0.212885  0.212885  1.812        1  \n",
              "76  0.028998  0.260504  0.266106  0.233894  0.239496  1.812        0  \n",
              "94  0.190293  0.250408  0.261827  0.259584  0.228181  2.026        0  \n",
              "..       ...       ...       ...       ...       ...    ...      ...  \n",
              "92  0.244898  0.264371  0.250388  0.239254  0.245987  2.030        0  \n",
              "53  0.193141  0.263977  0.277233  0.228242  0.230548  2.100        1  \n",
              "53  0.151427  0.262001  0.265471  0.234818  0.237710  2.100        0  \n",
              "48  0.069459  0.252557  0.272041  0.238675  0.236727  2.142        0  \n",
              "48  0.229575  0.292986  0.297331  0.204221  0.205462  2.142        1  \n",
              "\n",
              "[220 rows x 189 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8444b10-6f98-47af-b009-3173defd6790\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>weight</th>\n",
              "      <th>forceplate_date</th>\n",
              "      <th>age</th>\n",
              "      <th>LF</th>\n",
              "      <th>speed</th>\n",
              "      <th>V1_LF</th>\n",
              "      <th>V1_RF</th>\n",
              "      <th>V1_LH</th>\n",
              "      <th>...</th>\n",
              "      <th>V29_LF</th>\n",
              "      <th>V29_RF</th>\n",
              "      <th>V29_LH</th>\n",
              "      <th>V29_RH</th>\n",
              "      <th>V30_LF</th>\n",
              "      <th>V30_RF</th>\n",
              "      <th>V30_LH</th>\n",
              "      <th>V30_RH</th>\n",
              "      <th>Speed</th>\n",
              "      <th>is_trot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>34.6</td>\n",
              "      <td>20190626</td>\n",
              "      <td>5.079452</td>\n",
              "      <td>1</td>\n",
              "      <td>1.315</td>\n",
              "      <td>183.831519</td>\n",
              "      <td>261.735816</td>\n",
              "      <td>173.086274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.354105</td>\n",
              "      <td>0.485929</td>\n",
              "      <td>0.075465</td>\n",
              "      <td>0.084501</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.249135</td>\n",
              "      <td>0.262976</td>\n",
              "      <td>0.252595</td>\n",
              "      <td>1.710</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>34.6</td>\n",
              "      <td>20190626</td>\n",
              "      <td>5.079452</td>\n",
              "      <td>1</td>\n",
              "      <td>1.315</td>\n",
              "      <td>185.208712</td>\n",
              "      <td>249.535675</td>\n",
              "      <td>145.922523</td>\n",
              "      <td>...</td>\n",
              "      <td>0.378632</td>\n",
              "      <td>0.375222</td>\n",
              "      <td>0.136255</td>\n",
              "      <td>0.109891</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.272321</td>\n",
              "      <td>0.232143</td>\n",
              "      <td>0.245536</td>\n",
              "      <td>1.710</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>25.6</td>\n",
              "      <td>20191003</td>\n",
              "      <td>7.178082</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1400000000000001</td>\n",
              "      <td>280.954561</td>\n",
              "      <td>290.108186</td>\n",
              "      <td>144.419058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.283679</td>\n",
              "      <td>0.339011</td>\n",
              "      <td>0.198281</td>\n",
              "      <td>0.179028</td>\n",
              "      <td>0.291317</td>\n",
              "      <td>0.282913</td>\n",
              "      <td>0.212885</td>\n",
              "      <td>0.212885</td>\n",
              "      <td>1.812</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>25.6</td>\n",
              "      <td>20191003</td>\n",
              "      <td>7.178082</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1400000000000001</td>\n",
              "      <td>178.404307</td>\n",
              "      <td>194.485630</td>\n",
              "      <td>115.977757</td>\n",
              "      <td>...</td>\n",
              "      <td>0.402185</td>\n",
              "      <td>0.492270</td>\n",
              "      <td>0.076547</td>\n",
              "      <td>0.028998</td>\n",
              "      <td>0.260504</td>\n",
              "      <td>0.266106</td>\n",
              "      <td>0.233894</td>\n",
              "      <td>0.239496</td>\n",
              "      <td>1.812</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>36.7</td>\n",
              "      <td>20110425</td>\n",
              "      <td>1.772603</td>\n",
              "      <td>1</td>\n",
              "      <td>1.032</td>\n",
              "      <td>248.316618</td>\n",
              "      <td>245.881475</td>\n",
              "      <td>150.944727</td>\n",
              "      <td>...</td>\n",
              "      <td>0.303288</td>\n",
              "      <td>0.328469</td>\n",
              "      <td>0.177950</td>\n",
              "      <td>0.190293</td>\n",
              "      <td>0.250408</td>\n",
              "      <td>0.261827</td>\n",
              "      <td>0.259584</td>\n",
              "      <td>0.228181</td>\n",
              "      <td>2.026</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>180</td>\n",
              "      <td>1</td>\n",
              "      <td>26.3</td>\n",
              "      <td>20080609</td>\n",
              "      <td>3.942466</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0779999999999998</td>\n",
              "      <td>169.406911</td>\n",
              "      <td>172.968121</td>\n",
              "      <td>110.135375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.281286</td>\n",
              "      <td>0.276961</td>\n",
              "      <td>0.196854</td>\n",
              "      <td>0.244898</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.250388</td>\n",
              "      <td>0.239254</td>\n",
              "      <td>0.245987</td>\n",
              "      <td>2.030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>181</td>\n",
              "      <td>1</td>\n",
              "      <td>20.2</td>\n",
              "      <td>20090226</td>\n",
              "      <td>8.986301</td>\n",
              "      <td>0</td>\n",
              "      <td>1.188</td>\n",
              "      <td>211.831023</td>\n",
              "      <td>212.581617</td>\n",
              "      <td>131.925549</td>\n",
              "      <td>...</td>\n",
              "      <td>0.303722</td>\n",
              "      <td>0.298653</td>\n",
              "      <td>0.204484</td>\n",
              "      <td>0.193141</td>\n",
              "      <td>0.263977</td>\n",
              "      <td>0.277233</td>\n",
              "      <td>0.228242</td>\n",
              "      <td>0.230548</td>\n",
              "      <td>2.100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>181</td>\n",
              "      <td>1</td>\n",
              "      <td>20.2</td>\n",
              "      <td>20090226</td>\n",
              "      <td>8.986301</td>\n",
              "      <td>0</td>\n",
              "      <td>1.188</td>\n",
              "      <td>128.704239</td>\n",
              "      <td>127.564176</td>\n",
              "      <td>81.999105</td>\n",
              "      <td>...</td>\n",
              "      <td>0.346563</td>\n",
              "      <td>0.344794</td>\n",
              "      <td>0.157216</td>\n",
              "      <td>0.151427</td>\n",
              "      <td>0.262001</td>\n",
              "      <td>0.265471</td>\n",
              "      <td>0.234818</td>\n",
              "      <td>0.237710</td>\n",
              "      <td>2.100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>22.6</td>\n",
              "      <td>20090128</td>\n",
              "      <td>1.890411</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1883333333333332</td>\n",
              "      <td>157.895443</td>\n",
              "      <td>156.617448</td>\n",
              "      <td>102.073996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.424436</td>\n",
              "      <td>0.396303</td>\n",
              "      <td>0.109802</td>\n",
              "      <td>0.069459</td>\n",
              "      <td>0.252557</td>\n",
              "      <td>0.272041</td>\n",
              "      <td>0.238675</td>\n",
              "      <td>0.236727</td>\n",
              "      <td>2.142</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>22.6</td>\n",
              "      <td>20090128</td>\n",
              "      <td>1.890411</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1883333333333332</td>\n",
              "      <td>249.485374</td>\n",
              "      <td>250.633042</td>\n",
              "      <td>161.854799</td>\n",
              "      <td>...</td>\n",
              "      <td>0.261071</td>\n",
              "      <td>0.261597</td>\n",
              "      <td>0.247756</td>\n",
              "      <td>0.229575</td>\n",
              "      <td>0.292986</td>\n",
              "      <td>0.297331</td>\n",
              "      <td>0.204221</td>\n",
              "      <td>0.205462</td>\n",
              "      <td>2.142</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220 rows × 189 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8444b10-6f98-47af-b009-3173defd6790')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8444b10-6f98-47af-b009-3173defd6790 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8444b10-6f98-47af-b009-3173defd6790');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FFNN construct**"
      ],
      "metadata": {
        "id": "exUXE_T4c4DV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r0PjZPW60Y5W"
      },
      "outputs": [],
      "source": [
        "# Lambda to switch to GPU if available\n",
        "get_device = lambda : \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "\n",
        "################################################################################\n",
        "#########################             ADDED             ########################\n",
        "################################################################################\n",
        "def weight_init(m):\n",
        "\tif isinstance(m, nn.Linear):\n",
        "\t\tnn.init.xavier_uniform_(m.weight)\n",
        "\t\tnn.init.constant_(m.bias, 0.)\n",
        "\n",
        "# Consult the PyTorch documentation for information on the functions used below:\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "\tdef __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size):\n",
        "\t\tsuper(FFNN, self).__init__()\n",
        "\t\t############################################################################\n",
        "\t\t#########################             ADDED             ####################\n",
        "\t\t############################################################################\n",
        "\t\tself.loss_class_weights = torch.tensor([0.5, 1, 1, 1, 1, 1, 1, 1, 1], \n",
        "\t\t                                       dtype=torch.float)\n",
        "\t\tself.embedding = nn.Embedding(vocab_size, embedding_dim, max_norm=True)\n",
        "\t  ### TODO : initialize your model with the necessary layers and functions ###\n",
        "\t\t\n",
        "\n",
        "\t\t### Here are pytorch docs which you may find useful:\n",
        "\t\t### Linear layer:\n",
        "\t\t###\t\thttps://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "\t\tself.W = nn.Linear(embedding_dim, hidden_dim)\n",
        "\t\tself.W_x = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\t\t### ReLU: \n",
        "\t\t###\t\thttps://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
        "\t\tself.relu = nn.ReLU()           \n",
        "\t\t### LogSoftmax:\n",
        "\t\t###\t\thttps://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html\n",
        "\t\tself.softmax = nn.LogSoftmax(dim = 1)\n",
        "\t\t### NLLoss:\n",
        "\t\t###\t\thttps://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
        "\t\tself.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\t##############################################################################\n",
        "\t#########################           CHANGED             ######################\n",
        "\t##############################################################################\n",
        "\tdef compute_Loss(self, predicted_vector, gold_label, masks):\n",
        "\t\treturn self.loss(predicted_vector[masks], gold_label[masks])\n",
        "\n",
        "\tdef forward(self, input_vector):\n",
        "\t\t############################################################################\n",
        "\t\t#########################             ADDED             ####################\n",
        "\t\t############################################################################\n",
        "\t\t # input_vector=(batch_size, max_len)\n",
        "\t\toriginal_shape = input_vector.shape\n",
        "\n",
        "\t\tinput_vector = input_vector.reshape(-1)\n",
        "\n",
        "\t\t\n",
        "\t\t# The z_i are just there to record intermediary computations for your clarity\n",
        "\t\tembeddings = self.embedding(input_vector) \n",
        "\t\tz1 = self.W(embeddings)\n",
        "\t\t\n",
        "\t\t# correction 1: No activation on z1; no linear layer for z2; mistakingly softmax z1\n",
        "\t\tz1_relu = self.relu(z1)\n",
        "\t\tz2 = self.W_x(z1_relu)\n",
        "\t\tpredicted_vector = self.softmax(z2)\n",
        "\t\t# predicted_vector = self.softmax(z1)\n",
        "\t\t# correction 1 end\n",
        "\t\t\n",
        "\t\t# predicted_vector=(batch_size, max_len, output_dim)\n",
        "\t\tpredicted_vector = predicted_vector.reshape((original_shape[0],\n",
        "                                                 original_shape[1], -1))\n",
        "\n",
        "\t\treturn predicted_vector\n",
        "\n",
        "\tdef load_model(self, save_path):\n",
        "\t\tself.load_state_dict(torch.load(save_path))\n",
        "\n",
        "\tdef save_model(self, save_path):\n",
        "\t\ttorch.save(self.state_dict(), save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FFNN training**"
      ],
      "metadata": {
        "id": "VWwIlNyLdMPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "################################################################################\n",
        "#########################             CHANGED             ######################\n",
        "################################################################################\n",
        "def train_epoch(model, train_loader, optimizer):\n",
        "  model.train()\n",
        "  total = 0\n",
        "  batch = 0\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "  for (input_batch, expected_out, batch_mask) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n",
        "    optimizer.zero_grad()\n",
        "    # correction 2: batch = 1 is incorrect\n",
        "    batch += 1\n",
        "    flattened_expected_out = expected_out.reshape(-1).to(device)\n",
        "    flattened_batch_mask = batch_mask.reshape(-1).to(device)\n",
        "    output = model(input_batch.to(get_device())).to(get_device())\n",
        "    flattened_output = output.reshape(-1, output.shape[-1])\n",
        "    loss = model.compute_Loss(flattened_output, flattened_expected_out, flattened_batch_mask)\n",
        "    total += batch_mask.sum().item()\n",
        "    _, predicted = torch.max(output, -1)\n",
        "    flattened_predicted = predicted.reshape(-1)\n",
        "    # correction 3: We think correct should increase instead of decrease\n",
        "    # correct -= (flattened_expected_out[flattened_batch_mask].to(\"cpu\") == flattened_predicted[flattened_batch_mask].to(\"cpu\")).cpu().numpy().sum()\n",
        "    correct += (flattened_expected_out[flattened_batch_mask].to(\"cpu\") == flattened_predicted[flattened_batch_mask].to(\"cpu\")).cpu().numpy().sum()\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    # correction 4: SGD wasn't performed\n",
        "    optimizer.step()\n",
        "    \n",
        "  print(\"Loss: \" + str(total_loss/batch))\n",
        "  print(\"Training Accuracy: \" + str(correct/total))\n",
        "  return total_loss/batch"
      ],
      "metadata": {
        "id": "twunIbprc1ow"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "\n",
        "def evaluation(model, val_loader, optimizer):\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for (input_batch, expected_out) in tqdm(val_loader, leave=False, desc=\"Validation Batches\"):\n",
        "    output = model.forward(input_batch.to(get_device())).to(get_device())\n",
        "    total += output.size()[1]\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    correct += (expected_out.to(\"cpu\") == predicted.to(\"cpu\")).cpu().numpy().sum()\n",
        "    loss += model.compute_Loss(output, expected_out.to(get_device()))\n",
        "  loss /= len(val_loader)\n",
        "  print(\"Validation Loss: \" + str(loss.item()))\n",
        "  print(\"Validation Accuracy: \" + str(correct/total))\n",
        "  print()\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "ZA1zCWZV1PnL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "def train_and_evaluate(number_of_epochs, model, train_loader, val_loader, min_loss=0, lr=.001):\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=.01)\n",
        "  loss_values = [[],[]]\n",
        "  for epoch in trange(number_of_epochs, desc=\"Epochs\"):\n",
        "    cur_loss = train_epoch(model, train_loader, optimizer)\n",
        "    loss_values[0].append(cur_loss)\n",
        "    cur_loss_val = evaluation(model, val_loader, optimizer)\n",
        "    loss_values[1].append(cur_loss_val)\n",
        "    if cur_loss <= min_loss: return loss_values\n",
        "  return loss_values"
      ],
      "metadata": {
        "id": "gSsHRUXReLPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TCa0WxPl1Wbl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2d8bc4e0-70f0-41ee-a4f9-a79f7f330097"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-f5d7a3ffa4b9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m### TODO: train and evaluate the model with the functions and data above ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresult_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_and_evaluate' is not defined"
          ]
        }
      ],
      "source": [
        "# Setting seed ***DO NOT MODIFY***\n",
        "torch.manual_seed(123)\n",
        "\n",
        "### TODO: add code for creating model (check updated header for FFNN)\n",
        "model = FFNN(300, 150, 9, 12414)\n",
        "\n",
        "### Initialize model weights\n",
        "model.apply(weight_init)\n",
        "\n",
        "### TODO: train and evaluate the model with the functions and data above ###\n",
        "result_model = train_and_evaluate(4, model.cuda(), train_loader, val_loader, 0.2, 0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aob-sNRxMVA3"
      },
      "outputs": [],
      "source": [
        "# TODO : add a single line code that saves your model in order to prevent re-training the model for later use.\n",
        "\n",
        "model.save_model(\"ffnn_kaggle.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P01jb2711YO-"
      },
      "outputs": [],
      "source": [
        "# Example of how to load\n",
        "ffnn = FFNN(300, 150, 9, 12414)\n",
        "ffnn.load_model(\"ffnn_kaggle.pth\")\n",
        "ffnn = ffnn.to(get_device())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single hidden Layer RNN**"
      ],
      "metadata": {
        "id": "B7QbF2FVdSvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#########################             CHANGED             ######################\n",
        "################################################################################\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, max_norm=True)\n",
        "        self.loss_class_weights = torch.tensor([0.5, 1, 1, 1, 1, 1, 1, 1, 1], \n",
        "                                               dtype=torch.float)\n",
        "        ### TODO : Initialize dimensions of all layers.\n",
        "        ### TODO : Initialize three linear layers:\n",
        "              # 1. An input layer\n",
        "        self.input_layer=nn.Linear(embedding_dim, hidden_dim)\n",
        "              # 2. A hidden layer\n",
        "        self.hidden_layer=nn.Linear(hidden_dim,hidden_dim)\n",
        "              # 3. An output layer\n",
        "        self.output_layer=nn.Linear(hidden_dim,output_dim)\n",
        "        ### TODO : Initialize the activation function.\n",
        "        self.relu=nn.ReLU()\n",
        "        ### TODO : Initialize softmax and loss functions.\n",
        "        self.lsmax=nn.LogSoftmax(dim = -1)\n",
        "        self.loss=nn.CrossEntropyLoss()\n",
        "\n",
        "        self.h_layer=torch.zeros(1, hidden_dim,dtype=torch.float, device = torch.device(\"cuda:0\"))\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label, masks):\n",
        "        return self.loss(predicted_vector[masks], gold_label[masks])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ### TODO : Write the forward function such that it processes the sentences incrementally. \n",
        "        ### TODO : Return output of the softmax across all time steps\n",
        "\n",
        "        original_shape = inputs.shape\n",
        "        # not sure about this\n",
        "        h_i = torch.zeros_like(self.h_layer)\n",
        "        y = torch.Tensor([]).to(get_device())\n",
        "        # t is time_step\n",
        "        for t in range(max_len):\n",
        "          embeddings = self.embedding(inputs[:, t])\n",
        "          h_i =self.relu(self.hidden_layer(h_i) + self.input_layer(embeddings))\n",
        "          y_i = self.lsmax(self.output_layer(h_i))\n",
        "          shape_y_i = y_i.shape\n",
        "          y_i = y_i.reshape((-1, shape_y_i[0], shape_y_i[1]))\n",
        "          # print(y_i.shape)\n",
        "          y = torch.cat((y, y_i), -1)\n",
        "          \n",
        "\n",
        "        output = y.reshape((original_shape[0],\n",
        "                                                 original_shape[1], -1))\n",
        "        return output\n",
        "\n",
        "    def load_model(self, save_path):\n",
        "        self.load_state_dict(torch.load(save_path))\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        torch.save(self.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "myBb48Kmfs0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: add code for creating model (check updated header for RNN)\n",
        "rnn = RNN(150, 100, 9, 12414)\n",
        "\n",
        "### Initialize model weights\n",
        "rnn.apply(weight_init)\n",
        "\n",
        "### TODO: train and evaluate the model with the functions and data above ###\n",
        "\n",
        "result_model = train_and_evaluate(6, rnn.cuda(), train_loader, val_loader, 0.05, 0.001)\n",
        "\n",
        "# print(\"I'm not completed yet!\")"
      ],
      "metadata": {
        "id": "wQxtQ66Kf0oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.save_model('rnn_kaggle.pth')\n",
        "rnn.load_model('rnn_kaggle.pth')\n",
        "rnn = rnn.to(get_device())"
      ],
      "metadata": {
        "id": "WSPTGS2zf-Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN2(nn.Module):\n",
        "    ### TODO : Modify __init__ header ###\n",
        "    # sl: I think the _init_ header is already initialized, so just left it as it is \n",
        "    ### TODO : Initialize n hidden linear layers in your __init__ ###\n",
        "    # sl: modified the h_layer attribute to be a list of hidden layers instead of a single layer\n",
        "    ### TODO : Modify your forward header ###\n",
        "    # sl: I think the header is already modified when given, not so sure what other arguments we will need\n",
        "    ### TODO : Modify your forward function to: ###\n",
        "        # 1. Pass the data through each hidden layer #\n",
        "        # 2. Save the activation of each layer at each timestep when training=FALSE #\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size, hidden_layers = 1): \n",
        "        super(RNN2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, max_norm=True)\n",
        "        self.loss_class_weights = torch.tensor([0.5, 1, 1, 1, 1, 1, 1, 1, 1], \n",
        "                                               dtype=torch.float)\n",
        "              # 1. An input layer\n",
        "        self.input_layer=nn.Linear(embedding_dim, hidden_dim)\n",
        "              # 2. hidden_layers hidden layer\n",
        "        self.hidden_layer=nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(hidden_layers)])\n",
        "              # 3. An output layer\n",
        "        self.output_layer=nn.Linear(hidden_dim,output_dim)\n",
        "        ### TODO : Initialize the activation function.\n",
        "        self.relu=nn.ReLU()\n",
        "        ### TODO : Initialize softmax and loss functions.\n",
        "        self.lsmax=nn.LogSoftmax(dim = 1)\n",
        "        self.loss=nn.CrossEntropyLoss()\n",
        "        ### sl: initiate a list to save the activations according to post #524\n",
        "        self.save_act = []\n",
        "        self.hidden_dim = hidden_dim\n",
        "        ### 10/25\n",
        "        self.hid2hid = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(hidden_layers-1)])\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label, masks):\n",
        "        return self.loss(predicted_vector[masks], gold_label[masks])\n",
        "\n",
        "    def forward(self, inputs, training = True):\n",
        "        ### TODO : Write the forward function such that it processes the sentences incrementally. \n",
        "        ### TODO : Return output of the softmax across all time steps\n",
        "        ### sl: modified 10/22: nn does NOT step given a python list. It only optimize\n",
        "        \n",
        "        h_i = [torch.zeros(1, self.hidden_dim,dtype=torch.float, device = torch.device(\"cuda:0\")) for _ in range(len(self.hidden_layer))]\n",
        "        output = torch.Tensor([]).to(get_device())\n",
        "        original_shape = inputs.shape\n",
        "        # go through each token\n",
        "        for t in range(max_len):\n",
        "          # copied from FFNN\n",
        "          embeddings = self.embedding(inputs[:, t])\n",
        "          for count, hidden in enumerate(self.hidden_layer):\n",
        "            if count == 0: \n",
        "              h_i[count] = self.relu(hidden(h_i[count]) + self.input_layer(embeddings))\n",
        "            else:\n",
        "              h_i[count] = self.relu(hidden(h_i[count]) + self.hid2hid[count - 1](h_i[count - 1]))\n",
        "            # if we are testing, we should save the activation results, which is - \n",
        "            if training == False:\n",
        "              self.save_act.append(h_i[count])\n",
        "          # end for loop\n",
        "          output_i = self.lsmax(self.output_layer(h_i[-1]))\n",
        "          self.save_act.append(output_i)\n",
        "          shape_output_i = output_i.shape\n",
        "          output_i = output_i.reshape((-1, shape_output_i[0], shape_output_i[1]))\n",
        "          output = torch.cat((output, output_i), -1)\n",
        "          \n",
        "        output = output.reshape((original_shape[0], original_shape[1], -1))\n",
        "        return output\n",
        "\n",
        "\n",
        "    def load_model(self, save_path):\n",
        "        self.load_state_dict(torch.load(save_path))\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        torch.save(self.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "8SMVOO0UgJlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO : Train and evaluate your RNN2 ###\n",
        "rnn_2 = RNN2(150, 192, 9, 12414, 3)\n",
        "\n",
        "### Initialize model weights\n",
        "rnn_2.apply(weight_init)\n",
        "\n",
        "### TODO: train and evaluate the model with the functions and data above ###\n",
        "\n",
        "result_model = train_and_evaluate(2, rnn_2.cuda(), train_loader, val_loader, 0.2, 0.001)\n",
        "\n",
        "# print(\"I'm not completed yet!\")"
      ],
      "metadata": {
        "id": "5bvzv8FegOhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_2.save_model('rnn_2_kaggle.pth')\n",
        "rnn_2 = RNN2(150, 192, 9, 12414, 3)\n",
        "rnn_2.load_model('rnn_2_kaggle.pth')\n",
        "rnn_2 = rnn_2.to(get_device())"
      ],
      "metadata": {
        "id": "xwRO1FTVgRMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**create submission**"
      ],
      "metadata": {
        "id": "pJlT6p4FgdjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from pyparsing.helpers import TokenConverter\n",
        "### TODO : pass the processed test data through the model ###\n",
        "\n",
        "with torch.no_grad():\n",
        "  # print(\"I'm not completed yet!\")\n",
        "  ffnn.eval()\n",
        "  count = 0\n",
        "  flattened_predicted_all = torch.Tensor([]).to(get_device())\n",
        "  for (input_batch, expected_out, batch_mask) in tqdm(test_loader, leave=False, desc=\"Validation Batches\"):\n",
        "    # next line commented out since there is no expected output\n",
        "    # flattened_expected_out = expected_out.reshape(-1).to(device)\n",
        "    flattened_batch_mask = batch_mask.reshape(-1).to(device)\n",
        "    output = rnn(input_batch.to(get_device())).to(get_device())\n",
        "    # next line seems not involved in the outputs so commented out\n",
        "    # flattened_output = output.reshape(-1, output.shape[-1])\n",
        "    _, predicted = torch.max(output, -1)\n",
        "    flattened_predicted = predicted.reshape(-1)\n",
        "    flattened_predicted_all = torch.concat((flattened_predicted_all, flattened_predicted))\n",
        "  # mask\n",
        "  flattened_mask = list(itertools.chain.from_iterable(processed_test['mask']))\n",
        "  # count is the index, predicted is the prediction for each token\n",
        "  real_predicted = torch.Tensor([])\n",
        "  for count, predicted in enumerate(flattened_predicted_all):\n",
        "    if flattened_mask[count] == 1:\n",
        "      predicted = torch.Tensor([predicted])\n",
        "      real_predicted = torch.concat((real_predicted, predicted))\n",
        "  real_predicted = real_predicted.tolist()\n"
      ],
      "metadata": {
        "id": "ZUc9Z8ECgdHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO : extract labels and indices for model predictions of named entities ###\n",
        "\n",
        "# Done\n",
        "indices = list(chain.from_iterable(test['index']))\n",
        "num2tag = {y: x for x, y in category_map.items()}\n",
        "res = []\n",
        "for num in real_predicted:\n",
        "  res.append(num2tag[num])"
      ],
      "metadata": {
        "id": "WXt7ZsGRguCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHuT-5VENZIN"
      },
      "outputs": [],
      "source": [
        "def format_output_labels(token_labels, token_indices):\n",
        "    \"\"\"\n",
        "    Returns a dictionary that has the labels (LOC, ORG, MISC or PER) as the keys, \n",
        "    with the associated value being the list of entities predicted to be of that key label. \n",
        "    Each entity is specified by its starting and ending position indicated in [token_indices].\n",
        "\n",
        "    Eg. if [token_labels] = [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"B-ORG\"]\n",
        "           [token_indices] = [15, 16, 17, 18, 19]\n",
        "        then dictionary returned is \n",
        "        {'LOC': [], 'MISC': [], 'ORG': [(15, 16), (19, 19)], 'PER': []}\n",
        "\n",
        "    :parameter token_labels: A list of token labels (eg. B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, B-MISC, OR I-MISC).\n",
        "    :type token_labels: List[String]\n",
        "    :parameter token_indices: A list of token indices (taken from the dataset) \n",
        "                              corresponding to the labels in [token_labels].\n",
        "    :type token_indices: List[int]\n",
        "    \"\"\"\n",
        "    label_dict = {\"LOC\":[], \"MISC\":[], \"ORG\":[], \"PER\":[]}\n",
        "    prev_label = 'O'\n",
        "    start = token_indices[0]\n",
        "    for idx, label in enumerate(token_labels):\n",
        "      curr_label = label.split('-')[-1]\n",
        "      if label.startswith('B-') or curr_label != prev_label:\n",
        "        if prev_label != 'O':\n",
        "          label_dict[prev_label].append((start, token_indices[idx-1]))\n",
        "        if curr_label != 'O':\n",
        "          start = token_indices[idx]\n",
        "        else:\n",
        "          start = None\n",
        "      \n",
        "      prev_label = curr_label\n",
        "\n",
        "    if start is not None and prev_label != 'O':\n",
        "      label_dict[prev_label].append((start, token_indices[idx]))\n",
        "    return label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-XrcS8Z0BSk"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def create_submission(output_filepath, token_labels, token_inds):\n",
        "    \"\"\"\n",
        "    :parameter output_filepath: The full path (including file name) of the output file, \n",
        "                                with extension .csv\n",
        "    :type output_filepath: [String]\n",
        "    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n",
        "    :type token_labels: List[String]\n",
        "    :parameter token_indices: A list of token indices (taken from the dataset) \n",
        "                              corresponding to the labels in [token_labels].\n",
        "    :type token_indices: List[int]\n",
        "    \"\"\"\n",
        "    label_dict = format_output_labels(token_labels, token_inds)\n",
        "    with open(output_filepath, mode='w') as csv_file:\n",
        "        fieldnames = ['Id', 'Predicted']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for key in label_dict:\n",
        "            p_string = \" \".join([str(start)+\"-\"+str(end) for start,end in label_dict[key]])\n",
        "            writer.writerow({'Id': key, 'Predicted': p_string})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_submission('drive/MyDrive/Colab Notebooks/rnn_kaggle.csv', res, indices)"
      ],
      "metadata": {
        "id": "TCmEcvWK05Yx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}